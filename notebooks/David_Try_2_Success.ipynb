{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "# Math Question Answer Verification Competition\n",
    "\n",
    "**Goal**: Fine-tune Llama-3-8B model to predict if a given solution to a math problem is correct or not.\n",
    "\n",
    "**Try 2 Optimizations**:\n",
    "- **50,000 training samples** (5% of full dataset) with stratified split (99.5/0.5)\n",
    "- **LoRA rank 32** for good capacity\n",
    "- **Max sequence length 2048** to prevent truncation\n",
    "- **2 epochs** training (fast completion)\n",
    "- **Learning rate 2e-4** (vs 1e-4) for faster convergence ‚¨ÜÔ∏è\n",
    "- **Warmup 300 steps** (vs 500) for quick start ‚¨áÔ∏è\n",
    "- **Improved prompt template** emphasizing solution reasoning\n",
    "- **Constrained decoding** for reliable output parsing\n",
    "\n",
    "**Note**: This notebook is optimized for Google Colab with A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 40290,
     "status": "ok",
     "timestamp": 1762111750763,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "xStnwtpOqK0e"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9.]{3,}\", torch.__version__).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Hugging Face Login**\n",
    "\n",
    "For accessing gated models like Llama-3-8B, we need to authenticate with Hugging Face using your token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1762111751743,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "URSw7qlhqlgB",
    "outputId": "2a8125f0-898d-4fb5-8337-3f6b34b8b5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully logged in to Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face using token from Colab Secrets\n",
    "# Set HF_TOKEN in Colab Secrets (Secrets ‚Üí Add Secret)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token, add_to_git_credential=False)\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Warning: HF_TOKEN not found. Make sure to set it in Colab Secrets if the model is gated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Load the Model and Tokenizer**\n",
    "\n",
    "Load Llama-3-8B using Unsloth's FastLanguageModel with optimized settings:\n",
    "- **4-bit quantization**: Reduces GPU memory usage significantly\n",
    "- **Max sequence length 2048**: Prevents truncation of long problems\n",
    "- **Auto dtype detection**: Optimizes for your GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338,
     "referenced_widgets": [
      "3cfbadab57fe46f58d47aa577c873aa7",
      "8f5be6931f66494a80bb2865dc20180a",
      "0847d3ca31a44feebef0c23edc166f6b",
      "834d62b9cf6746d99a1fbd6db9b7fc23",
      "d5b504427bb7457facb184898236bb5c",
      "a39f9dd4a3974292b68e77b2f9d89e3a",
      "c1a0481fee4d4b7a83c17f3d43c6431c",
      "9fc3313be4794965b35f8c1a56bcd463",
      "28d8d33e77c94fb997af53f5b1cc3847",
      "217152bc012a40ccb9ecbb44dfef7a9c",
      "28318c87ae1e42c3a77a485c8acb3949",
      "0e05f6ed832d4c4ca79e3d6eb77429af",
      "ce66a3d727244ddea5f9c79f68733cae",
      "e2e1d62042954dd9bac57551e3e86b5c",
      "3156edb436584d159b827c41bf3bfc34",
      "3505709a94434a1d83703f0b881a2a54",
      "1c727bf4a541481d86a1464e92fda8d2",
      "d40ab643aae7437d8ebdd8a541179d37",
      "e2eb37db65fd432faf9569f3225a1c78",
      "0717f84248cc4bbebafb372fd777a789",
      "17ac6f227b894c82ad977d2ef7eb002e",
      "4d0ea282142c4c6eadfe319f69f2a6c5",
      "813c651b67b7496d84131892349e931c",
      "f895888ccb7a4637b28c8dbceae06342",
      "045c6685933946e19ec293a6b5d7c790",
      "b9a6dbd31c6d488c9bf64157477a212d",
      "def12a46414346f0965203d349103148",
      "45d271298b2d4c1491dd0fad81fc505f",
      "447dcb5e3ae7475090636001be42199b",
      "21fbc795e8df4d3cb2c4588416cf5a0a",
      "73f42f234ec74ac295b1c27f3cb99eed",
      "f33afb5dd6c04fba8ec028885ccf5885",
      "68d5f8b938664a5abc39c91a7f8de206",
      "833ae142d8164fa8be7ea5faaca888da",
      "6dac2eba76fd44c6a1988875965caa58",
      "68e71385834946239e2cb2d9bc7252bc",
      "c061faff31b4467ea22b1dac3d82b596",
      "25fe775eba364aaea2bcad1e8ac4391b",
      "eb97d7c0c0a349f7a145a6697ca36efb",
      "d6abde70c448427f90084751bcc2f924",
      "c82002aa4c4242cdb47ddbdcbfa91cca",
      "8f2f12a308424f078abd651b2acf1a38",
      "b683b39943084eb8a218d04d9ec53ffd",
      "78a57f956dee43c8a5e6c97ed9a84317",
      "8842dd28dfea4bf78a1a58ee4e215227",
      "39de5e5aa7bb492bbe2659f6f1f1e7cc",
      "2981b1d920b94212900f6c5fa050e561",
      "28a5e72bd1384c9da992e1e11a70afc2",
      "29401c31d0c34ccb826937f90526fdf9",
      "86a27ab8cd414328ae1b98742f88ca7d",
      "9e3640f7a31646be8e811f240d9c7a96",
      "77853097f19d4afb896039014663a54d",
      "3e8a2f06300d43409107387c22da541d",
      "b7b6ea2724b645f096fe3e8949fb02e5",
      "d5dee1de98a24bfdb50b99803544b2e8"
     ]
    },
    "executionInfo": {
     "elapsed": 77570,
     "status": "ok",
     "timestamp": 1762111829315,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "etaDwWGN3X7C",
    "outputId": "076e65ce-a65d-4aa6-9243-60cb535eee37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfbadab57fe46f58d47aa577c873aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e05f6ed832d4c4ca79e3d6eb77429af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813c651b67b7496d84131892349e931c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833ae142d8164fa8be7ea5faaca888da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8842dd28dfea4bf78a1a58ee4e215227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded with max_seq_length=2048\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Try 2 settings\n",
    "max_seq_length = 2048  # Increased from 1024 to prevent truncation\n",
    "dtype = None  # Auto-detect best data type for GPU\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "# Using Meta-Llama-3.1-8B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded with max_seq_length={max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5cL3djv3bRy",
    "outputId": "7c2d1a19-7f54-4de6-df74-b5b6df1f3c34"
   },
   "source": [
    "## **Step 4: Prepare the Dataset**\n",
    "\n",
    "This step prepares the training data for fine-tuning. It consists of three parts:\n",
    "\n",
    "1. **Load Dataset**: Load the full 1M training samples from Hugging Face\n",
    "2. **Split Dataset**: Create train/validation split using stratified sampling (maintains class balance)\n",
    "3. **Format Prompts**: Convert data into instructional prompts with improved structure\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Load and Split Dataset\n",
    "\n",
    "**Why Stratified Split?**\n",
    "- Ensures training and validation sets have the same True/False ratio\n",
    "- Prevents bias in validation metrics\n",
    "- Better representation of the overall dataset distribution\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Format Training Prompts\n",
    "\n",
    "**Key Improvements Over Baseline:**\n",
    "\n",
    "1. **Includes Student Answer**: The baseline prompt only had Question and Solution, but the task requires comparing the student's answer with the correct solution. This is critical!\n",
    "2. **Emphasizes Reasoning**: The prompt explicitly asks the model to analyze step-by-step reasoning from the solution\n",
    "3. **Clear Task Definition**: Better structure helps the model understand what it needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327,
     "referenced_widgets": [
      "4f886c30c7c84f0dbc0c546e48ed6969",
      "c64138ccdc0340a086c7d0cf06af9f78",
      "b6a99922fd34453b8f0bddc568156463",
      "9fa961f4b07241878e215ecf2e390607",
      "03a3332a57684fbfbbc787eeb0400f30",
      "00353a3ddb074fe78519dbd6757c04b0",
      "dc10ac497459455d8b58080d7b78d405",
      "bb52030f7f6a429091b0651e021ebe6d",
      "b6d048918ce946d0b0c20dd8d47bd346",
      "13663cba359347b8ae2862fa838362d2",
      "0239b846e8bf41a691e30c18568d7ff5",
      "aa8dc061ae34407a8f7edb4acf398947",
      "b80da3ce83544c4a93c88cdc1df5b5a0",
      "a9384aa8694f4129b9e36b12ca8a9c6d",
      "a008c24f86104aa1abfffc0dfda5fc0a",
      "15be7f8ef6e84f01b66702709213d539",
      "1e25b9b1251f4d9e85524bb22f849c83",
      "d65c81edba2a4a7db20ae1840129b87f",
      "e7445049edf8459cb8691f10277397b2",
      "7322ccb9e88b4fc7bd93f1e1326b3543",
      "92f83ee4e9884103bd8685b9b3393e5b",
      "4d1d89f512314e71828b0a2dfbd26ba8",
      "1cc521a6412646ae8ee447f5e35c29d3",
      "1425f873c1674501a37849072525477d",
      "ff5b79446bbd4e0c93e60da1301ce622",
      "90f3b72ad0834779be6794a7bd087df7",
      "7edc711dcd164d1aac814f655ceddca9",
      "8009d5681a9b4b2586169e226c45b7e3",
      "a07a2d956a4042b9b058aca52d46538f",
      "b6ac00b7db324e9c9e0c743deb87cfca",
      "2a52d83f34e6417a9999bc44a8974607",
      "f33a87b262cf4e688505fdb071cbe079",
      "b91b36c521a44422b9a8fdc4147d0886",
      "ac065adee949450489fcf08d37f1936b",
      "17ef73a0fa6b41ea84b1c7e2350efd4e",
      "b0f867ac3a9d473ab173ec5364388012",
      "e785863888514183a48ab8211f4b58f3",
      "83e00e9997874481a1a44d802f3f5936",
      "0d5f4be0f91e4bab895abf4bd7ef598d",
      "d2dd8884e2134e0fb1e4178c3366a6a2",
      "6a9257a3b2b743f3a4f3453707b818dd",
      "9c504e8609ad45e0b01e9dd0c77fc936",
      "16ea553983314953a419ddb2f8d575b9",
      "92a5d36eaa6a4d64b4215571658db6ec",
      "cb50778c993941a89c4e3fb99e7ae297",
      "b7e942f59c084a089beb243aa2df2e4e",
      "92e181957b744f02a605764f2dda73ec",
      "77591aff5d0646ccab112100d5372cfb",
      "c288b2cbcf6f486783789246db640108",
      "80d0e392aa474d39926b366e423475d6",
      "12037e61596445d598a23edff2e767a3",
      "1cd3ed4b088a465596a9a73a2474906c",
      "21a3a5bb7c1d42b9bd669bc97cbbf01a",
      "29b0fbe73d9c4f7fbe04138b8e9e62ae",
      "23b8155385ee47fbb49f3e4c124516e1",
      "403ffd1ad0d74c5786d10f36839eea17",
      "a517fa53c1024bc79cbfac8fb0ee4bd0",
      "dc1d194f73824a41a5a5c7e8f0f10d36",
      "081cc0c6e83741129a5240e4a26efc8f",
      "585f506995994a02a17b8437e889510c",
      "eb9158a1a49145f8845ea43d7533f6b4",
      "9cf345d0eb9449c2a5761c81827019b6",
      "ac552c4d368943eb9e1bcb1cbf7b3296",
      "504ca3c1bf2b4665bf53b32eaa26a8f2",
      "473aa25226b74d2c905efde67810831e",
      "aa7d871e17794354b593682417b4908d",
      "9b795e5148b948e38ebfafa2041b8bbb",
      "9fb495c8a17b4d92ad17ab3c8833251e",
      "c3cad5f3bbc54f1f8ed282d0b62b8711",
      "d2aafc776bad42a88e3a67851eb9f076",
      "a692769435c0465b855f50a69c9b4139",
      "b41408d84fb34b81ad8954dc570b06a5",
      "1f078ec0db85436480e9b5a6052d8977",
      "c667b05ac0c246609515af555dfc7c21",
      "926db4c6a1194500a64159287e928c81",
      "c7c5a6751ec84b2a90ab67bfabbad1f9",
      "45f37de325d74f0e8e5dafb6ad861531",
      "5eb5458f3c8f46f3ba00fe6d7f65c07e",
      "caeba4b8746d48f3aad7d135a64db48d",
      "64c0bb2888b5483fbee539ffed7632b2",
      "d8b0e436c4c14fbf962f09da6f8c91ba",
      "27b9c6cd76da4234aca27098185cf506",
      "7856c6dea993439ca90a34a9fb9966e4",
      "c52129b509574da99989b7c92b2cd8e5",
      "1f24f4a914c44bd8a11861f3f110b5e6",
      "cf265bdeeb2d481ba5d2bebc7eff89a7",
      "529ba83b464e47268cf059427244ae38",
      "1a5267367f3146d0a1f920b5e1e7ff89"
     ]
    },
    "executionInfo": {
     "elapsed": 39820,
     "status": "ok",
     "timestamp": 1762111869148,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "72i3wZVu_uJA",
    "outputId": "e498e1a7-77a1-4044-ca74-f58d08317715"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f886c30c7c84f0dbc0c546e48ed6969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8dc061ae34407a8f7edb4acf398947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc521a6412646ae8ee447f5e35c29d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac065adee949450489fcf08d37f1936b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb50778c993941a89c4e3fb99e7ae297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403ffd1ad0d74c5786d10f36839eea17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b795e5148b948e38ebfafa2041b8bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb5458f3c8f46f3ba00fe6d7f65c07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training samples: 49,750\n",
      "‚úÖ Validation samples: 250\n",
      "üõ°Ô∏è Try 1.5: Using 50,000 samples (5% of full dataset) for SAFE training\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full training dataset (1M samples)\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "\n",
    "# Encode is_correct as ClassLabel for stratified split\n",
    "# This ensures the train/val split maintains the same True/False ratio\n",
    "full_dataset = full_dataset.class_encode_column(\"is_correct\")\n",
    "\n",
    "# üõ°Ô∏è Try 2: Limit to 50,000 samples for SAFE & fast training (5% of full dataset)\n",
    "full_dataset = full_dataset.shuffle(seed=42).select(range(50000))\n",
    "\n",
    "# Stratified split: 99.5% training, 0.5% validation (maintains class balance)\n",
    "# Using smaller validation set for faster training\n",
    "split_dataset = full_dataset.train_test_split(\n",
    "    test_size=0.005,  # 0.5% for validation (~250 samples)\n",
    "    seed=42,\n",
    "    stratify_by_column=\"is_correct\"\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]  # ~49,750 samples\n",
    "validation_dataset = split_dataset[\"test\"]  # ~250 samples\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"‚úÖ Validation samples: {len(validation_dataset):,}\")\n",
    "print(f\"üõ°Ô∏è Try 1.5: Using 50,000 samples (5% of full dataset) for SAFE training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "246e4cf8a3b94b31a187698ade9dbfbe",
      "68a2fba7841745e998958d9c1d1d07fe",
      "1f5f288613804d56885e245726099208",
      "c052a92403ee494eaa10bf515d306eee",
      "7690aae461e447ff886997bc002e0ff7",
      "60470560c88148be81a52fad6874a92f",
      "aef268becf00436a8288d98ff329b5ef",
      "f88fb1a0a6c84476bce4bc44a9b35c59",
      "74a6f921923f4ab58d820f5ec26f155f",
      "fcae2cea761a4024a03cbfdd51de75bf",
      "3dc19d9c76d04bf18814d1fb4124faa4",
      "4d0ad5d51ac74dbcba68c3764505ae82",
      "f239ca1a7daa4e2e951d7cc36f8c3043",
      "c0ab428339d342f1a85c5b03109ad003",
      "bab31bd9a5334cbcba90ea22b6eaf300",
      "0ca596d5119d4928863ca25c4455d330",
      "3454151e7319407a90d2a2014e59fd59",
      "bba04b0dda5b4a4583d79cdef11a7397",
      "2806319f8dfa4088ab6c0e58c459a569",
      "f32eb8ca6921440bbf761bfceb2299c3",
      "d0d67011692742d58bef21f759b17f3e",
      "2e25170799e44069a261e5365d0159a3"
     ]
    },
    "executionInfo": {
     "elapsed": 1451,
     "status": "ok",
     "timestamp": 1762111870602,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "lEaRjozB3tz8",
    "outputId": "199bfbb6-b04e-4c81-dd08-42a18e85b02f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246e4cf8a3b94b31a187698ade9dbfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0ad5d51ac74dbcba68c3764505ae82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets formatted: 49,750 training, 250 validation\n"
     ]
    }
   ],
   "source": [
    "# Improved prompt template emphasizing solution reasoning\n",
    "training_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "{}\"\"\"\n",
    "\n",
    "# EOS token to mark completion\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Format data samples into the prompt template\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format dataset examples into training prompts.\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of dataset examples containing question, solution, answer, is_correct\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with formatted text prompts\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    answers = examples[\"answer\"]  # Student answers\n",
    "    outputs = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n",
    "        # Format the prompt with all components and add EOS token\n",
    "        text = training_prompt.format(\n",
    "            question,\n",
    "            str(solution),\n",
    "            str(answer),\n",
    "            str(output)\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Apply formatting to both training and validation datasets\n",
    "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "formatted_validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"‚úÖ Datasets formatted: {len(formatted_train_dataset):,} training, {len(formatted_validation_dataset):,} validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "## **Step 5: Configure LoRA**\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** allows us to efficiently fine-tune the model by training only a small number of adapter parameters instead of the full model.\n",
    "\n",
    "**Try 2 Settings**:\n",
    "- **Rank 32**: Good capacity for 50K dataset (balances speed, safety, and accuracy)\n",
    "- **Alpha 64**: Typically set to 2√órank for optimal scaling\n",
    "- **Dropout 0.05**: Light regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7753,
     "status": "ok",
     "timestamp": 1762111878360,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "yVZHQ4y74BCG",
    "outputId": "e3b6cd82-1e67-464b-d88f-9be3e420f8a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.10.12 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA with Try 2 parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,  # Good capacity\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64,  # Typically 2√órank\n",
    "    lora_dropout = 0.05,  # Light regularization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 6: Set Up SFTTrainer**\n",
    "\n",
    "Configure the training process with **Try 2 Training Settings**:\n",
    "- **2 epochs**: Full passes through 50K dataset for solid learning\n",
    "- **Learning rate 2e-4** (vs 1e-4): Higher LR for faster convergence on smaller dataset ‚¨ÜÔ∏è\n",
    "- **Warmup 300 steps** (vs 500): Shorter warmup to start learning faster ‚¨áÔ∏è\n",
    "- **No validation during training**: For maximum speed\n",
    "- **Save checkpoints at each epoch**: Keep last 2 checkpoints for model recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "f95e6a8344de41fc903cbf5be4e111d7",
      "597a270b0e4f4be286d0b00f78234be2",
      "019724780c3d455786de45493f25fa11",
      "7fcdbf6572914abab2396229b2326be3",
      "8d6350e50e834e248d16ada92674f863",
      "f6b998d67a2a46359d05b87c89f8f224",
      "b54f75f2592a4fe2b708c9551fbe5137",
      "30b5bc1dcdfd49c38087f8027ee4914c",
      "491846b52ba749748001c7a3731b88e3",
      "739f0dcd09a34441b71262288ea7e860",
      "19a9c2572cf64391a300a778d4595359"
     ]
    },
    "executionInfo": {
     "elapsed": 12646,
     "status": "ok",
     "timestamp": 1762111891023,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "YVrNhZ4y4zsK",
    "outputId": "69f8e5ca-8b5c-4e07-9214-87a83131fedc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95e6a8344de41fc903cbf5be4e111d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/49750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Try 1.5 Trainer configured: 50K samples, LR 2e-4, Warmup 300 (SAFE & FAST!)\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 300,  # Try 2: Reduced from 500 for faster start\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 2e-4,  # Try 2: Increased from 1e-4 for faster convergence\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 100,\n",
    "        eval_strategy = \"no\",  # Disable validation for maximum speed\n",
    "        save_strategy = \"epoch\",  # Save only at end of each epoch\n",
    "        save_total_limit = 2,  # Keep only 2 checkpoints\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Try 1.5 Trainer configured: 50K samples, LR 2e-4, Warmup 300 (SAFE & FAST!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 7: Start Training**\n",
    "\n",
    "Train the model for **2 epochs** over **49,750 samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 30745002,
     "status": "ok",
     "timestamp": 1762142636044,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "agvQR_Ku5wWY",
    "outputId": "6888f7bf-90f3-4703-d7cc-b30d974a920c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 49,750 | Num Epochs = 2 | Total steps = 12,438\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12438' max='12438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12438/12438 8:32:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.537900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.528600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.444500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.471500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.453700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.352500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.356900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.353900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.362200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.357500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.342300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.361700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.346400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.334900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.342700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.327100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.332700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.323000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.327800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.309800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12438, training_loss=0.435036080580161, metrics={'train_runtime': 30742.5538, 'train_samples_per_second': 3.237, 'train_steps_per_second': 0.405, 'total_flos': 1.8071066875691336e+18, 'train_loss': 0.435036080580161, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 8: Prepare for Inference**\n",
    "\n",
    "Prepare the trained model for faster inference and test on a validation example to verify it's working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1762142637070,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "1lvcDSh0JZYm",
    "outputId": "1177066c-91d6-4a72-f1b9-099a825f75f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### QUESTION ####\n",
      "Compute $\\cos 150^\\circ$.\n",
      "\n",
      "#### SOLUTION ####\n",
      "For this problem, we can simply rely on Python's mathematics libraries.\n",
      "<llm-code>\n",
      "from math import cos, radians\n",
      "\n",
      "rad = radians(150)\n",
      "cos(rad)\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "-0.8660254037844387\n",
      "</llm-co...\n",
      "\n",
      "#### STUDENT ANSWER ####\n",
      "-0.8660254037844387\n",
      "\n",
      "#### MODEL'S PREDICTION ####\n",
      "0<|end_of_text|>\n",
      "\n",
      "#### CORRECT ANSWER ####\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Improved inference prompt (matching training prompt structure)\n",
    "inference_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# Test on a validation example\n",
    "example = validation_dataset[10]\n",
    "question = example[\"question\"]\n",
    "solution = example[\"solution\"]\n",
    "answer = example[\"answer\"]\n",
    "\n",
    "# Format the prompt\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt.format(question, str(solution), str(answer))],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate prediction\n",
    "outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "# Display results\n",
    "print(\"#### QUESTION ####\")\n",
    "print(question)\n",
    "print(\"\\n#### SOLUTION ####\")\n",
    "print(solution[:200] + \"...\" if len(str(solution)) > 200 else solution)\n",
    "print(\"\\n#### STUDENT ANSWER ####\")\n",
    "print(answer)\n",
    "print(\"\\n#### MODEL'S PREDICTION ####\")\n",
    "output_part = response.split(\"Output:\\n\")[-1]\n",
    "print(output_part[:50])\n",
    "print(\"\\n#### CORRECT ANSWER ####\")\n",
    "print(example[\"is_correct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5489c0ed"
   },
   "source": [
    "## **Step 9: Generate Submission File**\n",
    "\n",
    "Generate predictions for all test samples and create the submission CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1911084,
     "status": "ok",
     "timestamp": 1762144548169,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "5e020e6b",
    "outputId": "f7de7f64-8839-458f-fac1-99e39f4d5359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 10,000 test samples...\n",
      "Using constrained decoding with allowed tokens: ['True', 'False', '1', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [31:50<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Submission file created successfully!\n",
      "   Total predictions: 10,000\n",
      "   True predictions: 3,042\n",
      "   False predictions: 6,958\n",
      "\n",
      "üìÅ File saved as 'submission.csv'\n",
      "   You can now download this file and submit it to the Kaggle competition.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Constrained Decoding: Force model to generate only \"True\"/\"False\" or \"1\"/\"0\" tokens\n",
    "class AllowedTokensLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"Logits Processor that forces model to generate only allowed tokens\"\"\"\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed = set(allowed_token_ids)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Set probability of disallowed tokens to -inf\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for tid in self.allowed:\n",
    "            if tid < scores.shape[-1]:\n",
    "                mask[..., tid] = 0.0\n",
    "        return scores + mask\n",
    "\n",
    "def get_allowed_token_ids(tokenizer):\n",
    "    \"\"\"Return token IDs for allowed tokens (True/False or 1/0)\"\"\"\n",
    "    tokens = [\"True\", \"False\", \"1\", \"0\"]\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id != tokenizer.unk_token_id:\n",
    "            ids.append(token_id)\n",
    "    return ids\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "predictions = []\n",
    "\n",
    "# Setup constrained decoding\n",
    "allowed_ids = get_allowed_token_ids(tokenizer)\n",
    "logits_proc = AllowedTokensLogitsProcessor(allowed_ids)\n",
    "\n",
    "# Optimized Generation configuration\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=1,        # Reduced from 8 to 1 (faster and more accurate)\n",
    "    do_sample=False,          # Deterministic generation\n",
    "    temperature=0.0,          # Use probability distribution as-is\n",
    "    top_p=1.0,\n",
    "    eos_token_id=[tokenizer.eos_token_id],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Generate predictions for all test samples\n",
    "print(f\"Generating predictions for {len(test_dataset):,} test samples...\")\n",
    "allowed_tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in allowed_ids if tid != tokenizer.unk_token_id]\n",
    "print(f\"Using constrained decoding with allowed tokens: {allowed_tokens}\")\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]  # Student answer\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = inference_prompt.format(question, str(solution), str(answer))\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate prediction with constrained decoding\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        logits_processor=[logits_proc],  # Apply constrained decoding\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    # Decode only newly generated tokens (more accurate parsing)\n",
    "    new_tokens = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Check first character only (0 or 1, True/False)\n",
    "    ch = response_text.strip()[:1] if response_text.strip() else \"0\"\n",
    "\n",
    "    # Parse: \"1\" or first letter \"T\" of \"True\" means True\n",
    "    if ch.lower() in [\"1\", \"t\"]:\n",
    "        prediction = True\n",
    "    else:\n",
    "        prediction = False\n",
    "\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission file created successfully!\")\n",
    "print(f\"   Total predictions: {len(predictions):,}\")\n",
    "print(f\"   True predictions: {sum(predictions):,}\")\n",
    "print(f\"   False predictions: {len(predictions) - sum(predictions):,}\")\n",
    "print(\"\\nüìÅ File saved as 'submission.csv'\")\n",
    "print(\"   You can now download this file and submit it to the Kaggle competition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgNtP84x_uJB"
   },
   "source": [
    "## **Step 10: Validate Submission File**\n",
    "\n",
    "Validate the submission CSV file format before submitting to Kaggle.\n",
    "\n",
    "**Checks**:\n",
    "- File has correct columns (`ID`, `is_correct`)\n",
    "- All values are `True` or `False`\n",
    "- No duplicate IDs\n",
    "- Exactly 10,000 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762144548206,
     "user": {
      "displayName": "David Hong",
      "userId": "09782621455553012703"
     },
     "user_tz": 300
    },
    "id": "Iv-Ttql0_uJC",
    "outputId": "40048164-2375-4eff-b9ec-dca1ba9ae4c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows:\n",
      "   ID  is_correct\n",
      "0   0       False\n",
      "1   1       False\n",
      "2   2       False\n",
      "3   3        True\n",
      "4   4       False\n",
      "\n",
      "Rows: 10000\n",
      "Unique IDs: 10000\n",
      "\n",
      "Value counts:\n",
      "is_correct\n",
      "False    6958\n",
      "True     3042\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Format validation passed!\n",
      "   Ready to submit to Kaggle competition.\n"
     ]
    }
   ],
   "source": [
    "# Validate submission file format\n",
    "# This helps catch errors before submitting to Kaggle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "df = pd.read_csv(submission_path)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check format\n",
    "print(f\"\\nRows: {len(df)}\")\n",
    "print(f\"Unique IDs: {df['ID'].nunique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(df['is_correct'].value_counts(dropna=False))\n",
    "\n",
    "# Validate format\n",
    "assert set(df['is_correct'].unique()).issubset({True, False}), \"is_correct must only contain True/False\"\n",
    "assert df['ID'].nunique() == len(df), \"ID column must have unique values\"\n",
    "assert len(df) == 10000, f\"Expected 10,000 rows, got {len(df)}\"\n",
    "\n",
    "print(\"\\n‚úÖ Format validation passed!\")\n",
    "print(\"   Ready to submit to Kaggle competition.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}