{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "# Math Question Answer Verification Competition\n",
    "\n",
    "**Goal**: Fine-tune Llama-3-8B model to predict if a given solution to a math problem is correct or not.\n",
    "\n",
    "**Optimizations Applied**:\n",
    "- Full 1M training dataset with stratified split (99.5/0.5) - 995K train, 5K validation\n",
    "- LoRA rank 32 (instead of 1) for better capacity\n",
    "- Max sequence length 2048 (instead of 1024) to prevent truncation\n",
    "- 2 epochs training without validation during training (for maximum speed)\n",
    "- Improved prompt template emphasizing solution reasoning\n",
    "- Constrained decoding for reliable output parsing\n",
    "\n",
    "**Note**: This notebook is optimized for Google Colab with A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xStnwtpOqK0e",
    "outputId": "ffdee8f3-3d91-48d9-86dd-2d4f951bf8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-_ffq9kqj/unsloth_01293ccc84294b4391e1905f721fe24a\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-_ffq9kqj/unsloth_01293ccc84294b4391e1905f721fe24a\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit f266b90718ff6e1d117c9707da9afac626a81e9d\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting unsloth_zoo>=2025.10.9 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2025.10.9-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.0)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.9.35-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets!=4.0.*,!=4.1.0,>=3.4.1 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.29.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.35.3)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
      "Collecting bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,<=4.56.2,>=4.51.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
      "Collecting torchao>=0.13.0 (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading torchao-0.14.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.10.1)\n",
      "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.23.0,>=0.7.9 (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.23.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.1)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0)\n",
      "Collecting msgspec (from unsloth_zoo>=2025.10.9->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.11.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.22.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,>=3.4.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.3/506.3 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth_zoo-2025.10.9-py3-none-any.whl (269 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.35-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading torchao-0.14.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.23.0-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2025.10.8-py3-none-any.whl size=348593 sha256=266f35669ad381743ffe5176a4805dac74c017c1975af4c9fa6bc713ebe68771\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-grrpaeu7/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: torchao, unsloth, shtab, pyarrow, msgspec, tyro, transformers, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo\n",
      "  Attempting uninstall: torchao\n",
      "    Found existing installation: torchao 0.10.0\n",
      "    Uninstalling torchao-0.10.0:\n",
      "      Successfully uninstalled torchao-0.10.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.1.0\n",
      "    Uninstalling pyarrow-18.1.0:\n",
      "      Successfully uninstalled pyarrow-18.1.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.0.0\n",
      "    Uninstalling datasets-4.0.0:\n",
      "      Successfully uninstalled datasets-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.1 cut_cross_entropy-25.1.1 datasets-4.2.0 msgspec-0.19.0 pyarrow-21.0.0 shtab-1.7.2 torchao-0.14.0 transformers-4.56.2 trl-0.23.0 tyro-0.9.35 unsloth-2025.10.8 unsloth_zoo-2025.10.9\n",
      "Collecting xformers<0.0.26\n",
      "  Downloading xformers-0.0.25.post1.tar.gz (4.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting trl<0.9.0\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft<0.12.0\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting accelerate<0.32.0\n",
      "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes<0.44.0\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting transformers<4.43.0\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: xformers\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for xformers\n",
      "Failed to build xformers\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (xformers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9.]{3,}\", torch.__version__).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Hugging Face Login**\n",
    "\n",
    "For accessing gated models like Llama-3-8B, we need to authenticate with Hugging Face using your token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361,
     "referenced_widgets": [
      "f1d1bd31aa6f468ca5b602f36f629b95",
      "6fa882a974a24de4a2493afaafa09444",
      "a82c5a30c053400fbbbe46507e008ece",
      "30280385aabf425388079d3fc8cb7e8e",
      "968c2ca4c4604a7da6316fcbf4726496",
      "3329c33438454b53b555a47852d1c659",
      "67c6e9f3350b470e819ef3572b495708",
      "9777cf15778e41da8c4917b8b77f1003",
      "7256ec7c0e724a16bf751e9d29b1948a",
      "e466ea8b194643838715e7b12122ee6b",
      "3a7b50fdc7da45bdaa967117c94f850f",
      "e075dfc764cd4be2aed8247491da6d10",
      "1b2903fcd665488e8dfbc5ffb7deade0",
      "edec4bb759eb49639736cf5ad6edf763",
      "002a020dcf064a368ac9003009c91c14",
      "3295712331b44605b70910c211f95eab",
      "6a63d68c3aaa470ea2005210a56a9e2c",
      "3d76d183bb59438ab082bef718768601",
      "87ed2439e5a14e58aaa5b62febf0d9f3",
      "d3e740573f814733be207816a5d7be84",
      "257c6645aa3a4788ba88ab381b04ee6e",
      "76115497e55549c28af2111ae6232d94",
      "665a5b1a75b245e38cf97f198ecacd5f",
      "0dc24eec009a44eca5839f84624e742f",
      "8b2c366dc0464d61bb4548cf16d8d4a8",
      "1049c755425b4691a0aed88ba6bd0473",
      "1d9c9d367e634742aaf2dadfb22e27a6",
      "dd98b696a08542b78075e3bd1be02ade",
      "0929c072bbe84b5d87d71f57a3b6b50f",
      "d9da202ca10c420588f62686e8020c10",
      "f2e4641da4044caa85e76e6027f6a055",
      "d733c4e0b0324067842750454a7965cf",
      "c90498c543784dd9823c4e9d3e6bfca9",
      "fce512b59cdb4315894c098d6ee9a257",
      "01dc304cabc143a08c3d9ba3d4a0ef51",
      "79965544bcd04d74a1e12c8f59c7669c",
      "78729a77a708424ea99a6ccec285584b",
      "3b5ccc2a6cd7459499b7046178e4209e",
      "b144ec950d744f879c1f3400a30041ef",
      "934ffdd954c1482eb13d844a0967f472",
      "a8d51257ea3f44a5b06feadbdeb157b1",
      "2d148a66fe8f4cec9d3e5b44f4293738",
      "67bd432820d947429ff5a262ca6705e9",
      "74063bdadccd43baa57c6308646a29d3",
      "098c2bd1e18341588fb539906c5c11e0",
      "094ecef2d8444f30b554257320c833a4",
      "7554de4e3063442a9e1f3a3dec85e859",
      "e9b0f0d8e0904b26b02647769d3b60ba",
      "8577e5df619546088ad3b34a09675d7f",
      "62d2d46f11e74818a835eaa69f23994b",
      "7892623a1c194df9b36b92ee789274be",
      "2c0185251e5c4c34a5a2acb1e3e17d0f",
      "69632f4a30194986b8d5dce6ef8b988e",
      "8ae5b3dc4e774822bcc11282aa14e34f",
      "8e07b18b341643b58df714febd86a7dd"
     ]
    },
    "id": "URSw7qlhqlgB",
    "outputId": "e94325f6-3aec-4ee9-eff7-7543b6a9617a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu126 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.8: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d1bd31aa6f468ca5b602f36f629b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e075dfc764cd4be2aed8247491da6d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665a5b1a75b245e38cf97f198ecacd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce512b59cdb4315894c098d6ee9a257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098c2bd1e18341588fb539906c5c11e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face using token from Colab Secrets\n",
    "# Set HF_TOKEN in Colab Secrets (Secrets â†’ Add Secret)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token, add_to_git_credential=False)\n",
    "    print(\"âœ… Successfully logged in to Hugging Face\")\n",
    "except:\n",
    "    print(\"âš ï¸ Warning: HF_TOKEN not found. Make sure to set it in Colab Secrets if the model is gated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Load the Model and Tokenizer**\n",
    "\n",
    "Load Llama-3-8B using Unsloth's FastLanguageModel with optimized settings:\n",
    "- **4-bit quantization**: Reduces GPU memory usage significantly\n",
    "- **Max sequence length 2048**: Prevents truncation of long problems\n",
    "- **Auto dtype detection**: Optimizes for your GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "325e5ac7457041cfbde440696b977a02",
      "706f9e73964b48038db8c2cdc0568a98",
      "2131e90e653c4132ad9248f71fa9d323",
      "e2972553424e41d0b24f94e9e76d6a2c",
      "a68936284859448aaa6879191c4c57d6",
      "d99521b4168949fbb79e5180419e2853",
      "85c2f528b23240b4a473a7b81d5869da",
      "1ab40058b92f40b090e42d4dd9adc9b3",
      "669aa4924b9d43409bb5860de238e2c4",
      "ecb50de04f39414289275f1431f6ed48",
      "981b5dcea9b54e478f4d475559f2f815",
      "499d86bff5564421bc21e50c9e24a6d1",
      "279f96260e3242b691e63dbf0ab7f146",
      "00002deddb8d49f1a9b7dbe2ebd71bc4",
      "84a8a2dab529456abb5e9b9e7ad30a49",
      "f23095ee5b0c4482acac28674d2bfcf9",
      "05775dd14def4e839752bece076fd079",
      "73dcdf3efe1040f6861c3c5f6b16ef45",
      "0ef97aca155640d7bde08fbd39bf5543",
      "4ac5fc173d204fdb95e5f9b0b467d450",
      "664ad0e6c179416a9ef2bfb741def1c0",
      "ebd7d071e0084a9b879d68098f40e305",
      "cf92542cab9e4d5c85f0a1da0deac6c4",
      "92d6406f40644d6d8e83daf3c965cfc8",
      "4c6f140c3fa549eeb997966ee7aeeef7",
      "55faa41a79f1459ca93598e6c697d57c",
      "15568810414a4696b7ce939e7b74b287",
      "84318abd3ad1479897a911a3b5c07d71",
      "5ca31b2223f741d5ab57395c0f3716df",
      "10ca4c4010764880997ef381d6a8431a",
      "3fe2863de22043fead20cc607707975c",
      "a1f8e43c9dff45caa9561537381ddd7f",
      "4a967d578f0c40dabfd59756d1004512",
      "ceede4d9c65e4887be1447d20ea317dc",
      "6f822fdb65b94c3d82fb12f71e7fa11e",
      "e3e418c2eeeb40f48683016a4464f7d2",
      "5f7905c6e9e940b698bad5586730d389",
      "b08054de7a51476fb48fb7ae2bb546d4",
      "24faddd3e2b34236b295ff634ed10ec6",
      "9c73c4ada19149248b039e97246da427",
      "13942cdcb4d04558987494f3e7247803",
      "13a11dbdd8774b75ba9caa77c2d57043",
      "640ac79895d8497c83f775a688a66031",
      "fdb81989e72d4802a39800eea3f92e5e",
      "041d02309517428486498564f0a0866c",
      "f8ffcfead53742f68e9d2889957a1170",
      "b06951c0a2c04ee78f0cd19bd469b4b8",
      "32ae30365ce04eaa860c28ce911ea472",
      "c3b7e75e515245869dfa792a63cc80f1",
      "51626651b35942d1a1356bf23173aa84",
      "34db018516a945fdaf3a8cfc0a7919a1",
      "b463babc27c246c48e5aa0c9e37e3619",
      "68d0a98e75554b4692f53b283a2f3328",
      "70fb6706ad5f4f6395c9cbb13f04fd69",
      "e249d026ed0b482186404d734c0867a9",
      "cbccdf8f7fc34b7f95865e95f253a259",
      "1997f19b001c4c23b28b2f2666d35ea3",
      "11d095f2cf974f509a0e78607d5bf183",
      "96be1314f19b4bc181d10c3eea00a210",
      "dab0b0c778a44220bddeb4991a575406",
      "ed459eaf5db049058ffa44ec59b80dfb",
      "dfe3bddec1364c78888057e493a68f81",
      "74deda38da3a4a12bac00043f5231fc2",
      "fcd9df5aa57a4eecbc1ae2d2eea35e2c",
      "191a5d2ae39143b7823d59881239c4ec",
      "167285cf97c24b798a113507da6ef2ca"
     ]
    },
    "id": "etaDwWGN3X7C",
    "outputId": "77b2b8b9-cad5-4672-ac47-fd66eb40f3d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325e5ac7457041cfbde440696b977a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499d86bff5564421bc21e50c9e24a6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf92542cab9e4d5c85f0a1da0deac6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceede4d9c65e4887be1447d20ea317dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041d02309517428486498564f0a0866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbccdf8f7fc34b7f95865e95f253a259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Optimized settings\n",
    "max_seq_length = 2048  # Increased from 1024 to prevent truncation\n",
    "dtype = None  # Auto-detect best data type for GPU\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "# Using Meta-Llama-3.1-8B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded with max_seq_length={max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "06e96f5dff98489a8abf78126d4daa24",
      "bc6d501aaf2f4a62a93f414a0ee68346",
      "7d8b892bbeb2463181d9f9fa83391e9c",
      "792cffb7ee72405ca21d23d428d5fe7b",
      "3551045ac12b4ee392c31c3f8392e4b9",
      "2316ac02e85644099cc9e13ea12635b9",
      "40b9755f2261461ab69928197fb04da3",
      "2d76a7f7fbe548788614f613bbd19d3f",
      "4a24b8cdfb364f0aa46aef5a65dc6c70",
      "84812dc0d17b4142bc120a60aa469ea0",
      "8e40206759ea48d083777c0d00144ae7"
     ]
    },
    "id": "i5cL3djv3bRy",
    "outputId": "7c2d1a19-7f54-4de6-df74-b5b6df1f3c34"
   },
   "source": [
    "## **Step 4: Prepare the Dataset**\n",
    "\n",
    "This step prepares the training data for fine-tuning. It consists of three parts:\n",
    "\n",
    "1. **Load Dataset**: Load the full 1M training samples from Hugging Face\n",
    "2. **Split Dataset**: Create train/validation split using stratified sampling (maintains class balance)\n",
    "3. **Format Prompts**: Convert data into instructional prompts with improved structure\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Load and Split Dataset\n",
    "\n",
    "**Why Stratified Split?**\n",
    "- Ensures training and validation sets have the same True/False ratio\n",
    "- Prevents bias in validation metrics\n",
    "- Better representation of the overall dataset distribution\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Format Training Prompts\n",
    "\n",
    "**Key Improvements Over Baseline:**\n",
    "\n",
    "1. **Includes Student Answer**: The baseline prompt only had Question and Solution, but the task requires comparing the student's answer with the correct solution. This is critical!\n",
    "2. **Emphasizes Reasoning**: The prompt explicitly asks the model to analyze step-by-step reasoning from the solution\n",
    "3. **Clear Task Definition**: Better structure helps the model understand what it needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full training dataset (1M samples)\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "\n",
    "# Encode is_correct as ClassLabel for stratified split\n",
    "# This ensures the train/val split maintains the same True/False ratio\n",
    "full_dataset = full_dataset.class_encode_column(\"is_correct\")\n",
    "\n",
    "# Stratified split: 99.5% training, 0.5% validation (maintains class balance)\n",
    "# Using smaller validation set for faster training\n",
    "split_dataset = full_dataset.train_test_split(\n",
    "    test_size=0.005,  # 0.5% for validation (5,000 samples)\n",
    "    seed=42,\n",
    "    stratify_by_column=\"is_correct\"\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]  # ~995,000 samples\n",
    "validation_dataset = split_dataset[\"test\"]  # ~5,000 samples\n",
    "\n",
    "print(f\"âœ… Training samples: {len(train_dataset):,}\")\n",
    "print(f\"âœ… Validation samples: {len(validation_dataset):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEaRjozB3tz8",
    "outputId": "13d7731b-da0f-4818-945e-cfaf8911c188"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.10.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# Improved prompt template emphasizing solution reasoning\n",
    "training_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "{}\"\"\"\n",
    "\n",
    "# EOS token to mark completion\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Format data samples into the prompt template\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format dataset examples into training prompts.\n",
    "    \n",
    "    Args:\n",
    "        examples: Batch of dataset examples containing question, solution, answer, is_correct\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with formatted text prompts\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    answers = examples[\"answer\"]  # Student answers\n",
    "    outputs = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n",
    "        # Format the prompt with all components and add EOS token\n",
    "        text = training_prompt.format(\n",
    "            question, \n",
    "            str(solution), \n",
    "            str(answer),\n",
    "            str(output)\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Apply formatting to both training and validation datasets\n",
    "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "formatted_validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"âœ… Datasets formatted: {len(formatted_train_dataset):,} training, {len(formatted_validation_dataset):,} validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "## **Step 5: Configure LoRA**\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** allows us to efficiently fine-tune the model by training only a small number of adapter parameters instead of the full model.\n",
    "\n",
    "**Optimized Settings**:\n",
    "- **Rank 32**: Increased from 1 for better model capacity\n",
    "- **Alpha 64**: Typically set to 2Ã—rank for optimal scaling\n",
    "- **Dropout 0.05**: Light regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8b577c76c81e4f77bc83303d326a16f4",
      "3ced5110eb7640ceba9e3e9b7529bec6",
      "b9701d93394b494ba74b4bc0969aa456",
      "db4653b181e442b1abc5bceaaaf299bb",
      "a34ca9ee00404e73a4a060f757691131",
      "a3d7af77bb0c401cadaec5c5d6f49505",
      "e8a51cf6c805466f845111a45b6470b1",
      "a368cd27ff8246a5a2f8547999ca89b5",
      "4eeeafb71d034ab2a2e23f9ead941bdb",
      "9e6668758e044356be6374391af102b7",
      "eae76f27702f46f58dfa4cba2293f4f4"
     ]
    },
    "id": "yVZHQ4y74BCG",
    "outputId": "05c23e82-0647-4a17-fde1-3be175faf6a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b577c76c81e4f77bc83303d326a16f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure LoRA with optimized parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,  # Increased from 1: more capacity for better performance\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64,  # Typically 2Ã—rank\n",
    "    lora_dropout = 0.05,  # Light regularization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 6: Set Up SFTTrainer**\n",
    "\n",
    "Configure the training process with:\n",
    "- **2 epochs**: Full passes through the 1M dataset (critical for high accuracy)\n",
    "- **Learning rate 1e-4**: Stable training for large dataset\n",
    "- **Warmup steps 500**: Gradual learning rate increase for stable start\n",
    "- **No validation during training**: Disabled for maximum training speed\n",
    "- **Save checkpoints at each epoch**: Keep last 2 checkpoints for model recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "YVrNhZ4y4zsK",
    "outputId": "a2e1c91b-46a4-40a7-81d5-872e869ad53a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 5,000 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 2,621,440 of 8,032,882,688 (0.03% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 10:05, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.467100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.901400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.771100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.9629756848017375, metrics={'train_runtime': 632.943, 'train_samples_per_second': 0.758, 'train_steps_per_second': 0.095, 'total_flos': 7549047826907136.0, 'train_loss': 0.9629756848017375, 'epoch': 0.096})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 500,\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 100,\n",
    "        eval_strategy = \"no\",  # Disable validation for maximum speed\n",
    "        save_strategy = \"epoch\",  # Save only at end of each epoch\n",
    "        save_total_limit = 2,  # Keep only 2 checkpoints\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured: 1M samples, 2 epochs, LR 1e-4, no validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 7: Start Training**\n",
    "\n",
    "Train the model for 2 epochs over the full 995K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agvQR_Ku5wWY",
    "outputId": "c276d985-ff48-4c2c-a7c4-fb105e274d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### QUESTION ####\n",
      "Jason and Jeremy want to paint their wall white and agreed to split the cost of the paint. A gallon of paint costs $45 and can cover up to 400 square feet. How much will each of them contribute to the cost of the paint if their walls have a total area of 1600 square feet and will need a second coat?\n",
      "\n",
      "#### SOLUTION ####\n",
      "The question asks how much each of them will pay for the paint. So let's first find the total cost of the paint. Let's solve this problem using Python code.\n",
      "<llm-code>\n",
      "# area of one gallon of paint\n",
      "area_per_gallon = 400\n",
      "# number of gallons\n",
      "gallons = 1600 / area_per_gallon\n",
      "# price per gallon\n",
      "price_per_gallon = 45\n",
      "# number of coats\n",
      "number_of_coats = 2\n",
      "cost_of_paint = gallons * number_of_coats * price_per_gallon\n",
      "cost_of_paint\n",
      "</llm-code>\n",
      "<llm-code-output>\n",
      "360.0\n",
      "</llm-code-output>\n",
      "Each will pay \\boxed{180} dollars.\n",
      "\n",
      "#### MODEL'S PREDICTION ####\n",
      "True<|end_of_text|>\n",
      "\n",
      "#### CORRECT ANSWER ####\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 8: Prepare for Inference**\n",
    "\n",
    "Prepare the trained model for faster inference and test on a validation example to verify it's working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lvcDSh0JZYm",
    "outputId": "f0c8b5ab-41ba-4fe7-aca9-56ff422ad485"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 1064/10000 [09:50<1:49:57,  1.35it/s]Unsloth: Input IDs of shape torch.Size([1, 1029]) with length 1029 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 13%|â–ˆâ–        | 1274/10000 [11:44<1:24:15,  1.73it/s]Unsloth: Input IDs of shape torch.Size([1, 1197]) with length 1197 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 16%|â–ˆâ–Œ        | 1572/10000 [14:31<1:21:11,  1.73it/s]Unsloth: Input IDs of shape torch.Size([1, 1038]) with length 1038 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 23%|â–ˆâ–ˆâ–       | 2329/10000 [21:37<1:15:35,  1.69it/s]Unsloth: Input IDs of shape torch.Size([1, 1211]) with length 1211 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 3050/10000 [28:20<59:13,  1.96it/s]  Unsloth: Input IDs of shape torch.Size([1, 1256]) with length 1256 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 3251/10000 [30:14<1:01:32,  1.83it/s]Unsloth: Input IDs of shape torch.Size([1, 1050]) with length 1050 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 3321/10000 [30:56<1:03:34,  1.75it/s]Unsloth: Input IDs of shape torch.Size([1, 1151]) with length 1151 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3547/10000 [33:00<52:32,  2.05it/s]Unsloth: Input IDs of shape torch.Size([1, 1306]) with length 1306 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 3988/10000 [37:03<52:02,  1.93it/s]Unsloth: Input IDs of shape torch.Size([1, 1061]) with length 1061 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4584/10000 [42:31<48:12,  1.87it/s]Unsloth: Input IDs of shape torch.Size([1, 1063]) with length 1063 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5412/10000 [50:09<38:38,  1.98it/s]Unsloth: Input IDs of shape torch.Size([1, 1320]) with length 1320 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5943/10000 [55:04<38:38,  1.75it/s]Unsloth: Input IDs of shape torch.Size([1, 1185]) with length 1185 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6441/10000 [59:36<38:59,  1.52it/s]Unsloth: Input IDs of shape torch.Size([1, 1076]) with length 1076 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6559/10000 [1:00:44<32:07,  1.78it/s]Unsloth: Input IDs of shape torch.Size([1, 1269]) with length 1269 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 6620/10000 [1:01:20<28:42,  1.96it/s]Unsloth: Input IDs of shape torch.Size([1, 1234]) with length 1234 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7763/10000 [1:11:46<19:17,  1.93it/s]Unsloth: Input IDs of shape torch.Size([1, 1031]) with length 1031 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 7964/10000 [1:13:37<20:51,  1.63it/s]Unsloth: Input IDs of shape torch.Size([1, 1158]) with length 1158 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 9673/10000 [1:29:13<02:43,  2.00it/s]Unsloth: Input IDs of shape torch.Size([1, 1104]) with length 1104 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 9765/10000 [1:30:08<02:08,  1.83it/s]Unsloth: Input IDs of shape torch.Size([1, 1047]) with length 1047 > the model's max sequence length of 1024.\n",
      "We shall truncate it ourselves. It's imperative if you correct this issue first.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [1:32:17<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission.csv' created successfully!\n",
      "You can now download this file and submit it to the Kaggle competition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Improved inference prompt (matching training prompt structure)\n",
    "inference_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# Test on a validation example\n",
    "example = validation_dataset[10]\n",
    "question = example[\"question\"]\n",
    "solution = example[\"solution\"]\n",
    "answer = example[\"answer\"]\n",
    "\n",
    "# Format the prompt\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt.format(question, str(solution), str(answer))],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate prediction\n",
    "outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "# Display results\n",
    "print(\"#### QUESTION ####\")\n",
    "print(question)\n",
    "print(\"\\n#### SOLUTION ####\")\n",
    "print(solution[:200] + \"...\" if len(str(solution)) > 200 else solution)\n",
    "print(\"\\n#### STUDENT ANSWER ####\")\n",
    "print(answer)\n",
    "print(\"\\n#### MODEL'S PREDICTION ####\")\n",
    "output_part = response.split(\"Output:\\n\")[-1]\n",
    "print(output_part[:50])\n",
    "print(\"\\n#### CORRECT ANSWER ####\")\n",
    "print(example[\"is_correct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5489c0ed"
   },
   "source": [
    "## **Step 9: Generate Submission File**\n",
    "\n",
    "Generate predictions for all test samples and create the submission CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e020e6b",
    "outputId": "fdb77cf8-fd0c-4e3b-92a0-e501f3c71be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Constrained Decoding: Force model to generate only \"True\"/\"False\" or \"1\"/\"0\" tokens\n",
    "class AllowedTokensLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"Logits Processor that forces model to generate only allowed tokens\"\"\"\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed = set(allowed_token_ids)\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Set probability of disallowed tokens to -inf\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for tid in self.allowed:\n",
    "            if tid < scores.shape[-1]:\n",
    "                mask[..., tid] = 0.0\n",
    "        return scores + mask\n",
    "\n",
    "def get_allowed_token_ids(tokenizer):\n",
    "    \"\"\"Return token IDs for allowed tokens (True/False or 1/0)\"\"\"\n",
    "    tokens = [\"True\", \"False\", \"1\", \"0\"]\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id != tokenizer.unk_token_id:\n",
    "            ids.append(token_id)\n",
    "    return ids\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "predictions = []\n",
    "\n",
    "# Setup constrained decoding\n",
    "allowed_ids = get_allowed_token_ids(tokenizer)\n",
    "logits_proc = AllowedTokensLogitsProcessor(allowed_ids)\n",
    "\n",
    "# Optimized Generation configuration\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=1,        # Reduced from 8 to 1 (faster and more accurate)\n",
    "    do_sample=False,          # Deterministic generation\n",
    "    temperature=0.0,          # Use probability distribution as-is\n",
    "    top_p=1.0,\n",
    "    eos_token_id=[tokenizer.eos_token_id],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Generate predictions for all test samples\n",
    "print(f\"Generating predictions for {len(test_dataset):,} test samples...\")\n",
    "allowed_tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in allowed_ids if tid != tokenizer.unk_token_id]\n",
    "print(f\"Using constrained decoding with allowed tokens: {allowed_tokens}\")\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]  # Student answer\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = inference_prompt.format(question, str(solution), str(answer))\n",
    "    inputs = tokenizer(\n",
    "        [prompt], \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate prediction with constrained decoding\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        logits_processor=[logits_proc],  # Apply constrained decoding\n",
    "        use_cache=True\n",
    "    )\n",
    "    \n",
    "    # Decode only newly generated tokens (more accurate parsing)\n",
    "    new_tokens = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Check first character only (0 or 1, True/False)\n",
    "    ch = response_text.strip()[:1] if response_text.strip() else \"0\"\n",
    "    \n",
    "    # Parse: \"1\" or first letter \"T\" of \"True\" means True\n",
    "    if ch.lower() in [\"1\", \"t\"]:\n",
    "        prediction = True\n",
    "    else:\n",
    "        prediction = False\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Submission file created successfully!\")\n",
    "print(f\"   Total predictions: {len(predictions):,}\")\n",
    "print(f\"   True predictions: {sum(predictions):,}\")\n",
    "print(f\"   False predictions: {len(predictions) - sum(predictions):,}\")\n",
    "print(\"\\nğŸ“ File saved as 'submission.csv'\")\n",
    "print(\"   You can now download this file and submit it to the Kaggle competition.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 10: Validate Submission File**\n",
    "\n",
    "Validate the submission CSV file format before submitting to Kaggle.\n",
    "\n",
    "**Checks**:\n",
    "- File has correct columns (`ID`, `is_correct`)\n",
    "- All values are `True` or `False`\n",
    "- No duplicate IDs\n",
    "- Exactly 10,000 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate submission file format\n",
    "# This helps catch errors before submitting to Kaggle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "submission_path = 'submission.csv'\n",
    "df = pd.read_csv(submission_path)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check format\n",
    "print(f\"\\nRows: {len(df)}\")\n",
    "print(f\"Unique IDs: {df['ID'].nunique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(df['is_correct'].value_counts(dropna=False))\n",
    "\n",
    "# Validate format\n",
    "assert set(df['is_correct'].unique()).issubset({True, False}), \"is_correct must only contain True/False\"\n",
    "assert df['ID'].nunique() == len(df), \"ID column must have unique values\"\n",
    "assert len(df) == 10000, f\"Expected 10,000 rows, got {len(df)}\"\n",
    "\n",
    "print(\"\\nâœ… Format validation passed!\")\n",
    "print(\"   Ready to submit to Kaggle competition.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}