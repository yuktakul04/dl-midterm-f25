{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "# Math Question Answer Verification Competition\n",
    "\n",
    "**Goal**: Fine-tune Llama-3-8B model to predict if a given solution to a math problem is correct or not.\n",
    "\n",
    "**Try 1 Optimizations**:\n",
    "- **45,000 training samples** (50,000 total with 5,000 validation, 5% of full dataset) with stratified split (90/10)\n",
    "- **LoRA rank 16** for good capacity\n",
    "- **Max sequence length 2048** to prevent truncation\n",
    "- **1 epoch** training (fast completion)\n",
    "- **Learning rate 2e-4** (vs 1e-4) for faster convergence ‚¨ÜÔ∏è\n",
    "- **Warmup 100 steps** (vs 500) for quick start ‚¨áÔ∏è\n",
    "- **Improved prompt template** emphasizing solution reasoning\n",
    "- **Constrained decoding** for reliable output parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xStnwtpOqK0e"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9.]{3,}\", torch.__version__).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Hugging Face Login**\n",
    "\n",
    "For accessing gated models like Llama-3-8B, we need to authenticate with Hugging Face using your token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URSw7qlhqlgB",
    "outputId": "2fb9381e-c6a0-4d32-ebba-8f65c1a53f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Warning: HF_TOKEN not found. Make sure to set it in Colab Secrets if the model is gated.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face using token from Colab Secrets\n",
    "# Set HF_TOKEN in Colab Secrets (Secrets ‚Üí Add Secret)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token, add_to_git_credential=False)\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Warning: HF_TOKEN not found. Make sure to set it in Colab Secrets if the model is gated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Load the Model and Tokenizer**\n",
    "\n",
    "Load Llama-3-8B using Unsloth's FastLanguageModel with optimized settings:\n",
    "- **4-bit quantization**: Reduces GPU memory usage significantly\n",
    "- **Max sequence length 2048**: Prevents truncation of long problems\n",
    "- **Auto dtype detection**: Optimizes for your GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etaDwWGN3X7C",
    "outputId": "54fffec1-4387-4984-c9e1-8ebbcc7c985a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Model loaded with max_seq_length=2048\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Optimized settings\n",
    "max_seq_length = 2048  # Increased from 1024 to prevent truncation\n",
    "dtype = None  # Auto-detect best data type for GPU\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "# Using Meta-Llama-3.1-8B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded with max_seq_length={max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5cL3djv3bRy",
    "outputId": "7c2d1a19-7f54-4de6-df74-b5b6df1f3c34"
   },
   "source": [
    "## **Step 4: Prepare the Dataset**\n",
    "\n",
    "This step prepares the training data for fine-tuning. It consists of three parts:\n",
    "\n",
    "1. **Load Dataset**: Load the full 1M training samples from Hugging Face\n",
    "2. **Split Dataset**: Create train/validation split using stratified sampling (maintains class balance)\n",
    "3. **Format Prompts**: Convert data into instructional prompts with improved structure\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Load and Split Dataset\n",
    "\n",
    "**Why Stratified Split?**\n",
    "- Ensures training and validation sets have the same True/False ratio\n",
    "- Prevents bias in validation metrics\n",
    "- Better representation of the overall dataset distribution\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Format Training Prompts\n",
    "\n",
    "**Key Improvements Over Baseline:**\n",
    "\n",
    "1. **Includes Student Answer**: The baseline prompt only had Question and Solution, but the task requires comparing the student's answer with the correct solution. This is critical!\n",
    "2. **Emphasizes Reasoning**: The prompt explicitly asks the model to analyze step-by-step reasoning from the solution\n",
    "3. **Clear Task Definition**: Better structure helps the model understand what it needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvAF1sBx8ksx",
    "outputId": "6f9028ce-45cd-430d-b9eb-e2682e6c3d5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training samples: 45,000\n",
      "‚úÖ Validation samples: 5,000\n",
      "‚ö° Try 1.5: Using 80,000 samples (8% of full dataset) for fast training\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full training dataset (1M samples)\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "\n",
    "# Encode is_correct as ClassLabel for stratified split\n",
    "# This ensures the train/val split maintains the same True/False ratio\n",
    "full_dataset = full_dataset.class_encode_column(\"is_correct\")\n",
    "\n",
    "# ‚ö° Try 1: Limit to 50,000 samples for faster training (5% of full dataset)\n",
    "full_dataset = full_dataset.shuffle(seed=42).select(range(50000))\n",
    "\n",
    "# Stratified split: 90% training, 10% validation (maintains class balance)\n",
    "# Using reasonable validation set for model evaluation\n",
    "split_dataset = full_dataset.train_test_split(\n",
    "    test_size=0.1,  # 10% for validation (5,000 samples)\n",
    "    seed=42,\n",
    "    stratify_by_column=\"is_correct\"\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]  # 45,000 samples\n",
    "validation_dataset = split_dataset[\"test\"]  # 5,000 samples\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"‚úÖ Validation samples: {len(validation_dataset):,}\")\n",
    "print(f\"‚ö° Try 1: Using 50,000 samples (5% of full dataset) for fast training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "e9d2b72b9b804681b7d1fdb9a3f589cd",
      "b74ba477e95a403ca0b83f8bb5a7df22",
      "9ef5ce9911b843f2a7ada5fa6c2d59ab",
      "5848433f03ff46e8b26a507015a312f4",
      "8875e2b78a9b4099bcd91433502b4ec6",
      "8e43adb8ab3e4d27996d1d5337d3a278",
      "64312b12c3c74895898bac49dfefd949",
      "babf13dee5784fc8833b88cb28d4f710",
      "8c2b7ccfc18c4ebcbfc701ecb9293ae3",
      "824292a2a9e94c68a084bf6f63009a97",
      "dbaef8e02d32436c8add15adb3231263",
      "c475a67aff3e40929303ca66ea03e8aa",
      "56aa306c97d047fabbda28257f6b4f9e",
      "ff464e157dcf4837ac576b954aab6df0",
      "1f2c9415d43745ff834d65fd4576cceb",
      "429605ad7f5f444a9d3810486dcaf5d5",
      "2cdf9e17403846db8fe343f5873b7ccb",
      "3c587e117b8349709be939868c2ed7f8",
      "3efdaae860a040e197836437d5a585b3",
      "b3e1935534514b5ebdb2bfab5f53bed4",
      "0e57ff27bc24426eb15e833651481a5e",
      "2b5b6a2e5b6840299ca270c666c20c81"
     ]
    },
    "id": "lEaRjozB3tz8",
    "outputId": "1bac12ed-f711-4fe7-d6d5-ab7d678b6733"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d2b72b9b804681b7d1fdb9a3f589cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c475a67aff3e40929303ca66ea03e8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets formatted: 45,000 training, 5,000 validation\n"
     ]
    }
   ],
   "source": [
    "# Improved prompt template emphasizing solution reasoning\n",
    "training_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "{}\"\"\"\n",
    "\n",
    "# EOS token to mark completion\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Format data samples into the prompt template\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format dataset examples into training prompts.\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of dataset examples containing question, solution, answer, is_correct\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with formatted text prompts\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    answers = examples[\"answer\"]  # Student answers\n",
    "    outputs = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n",
    "        # Format the prompt with all components and add EOS token\n",
    "        text = training_prompt.format(\n",
    "            question,\n",
    "            str(solution),\n",
    "            str(answer),\n",
    "            str(output)\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Apply formatting to both training and validation datasets\n",
    "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "formatted_validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"‚úÖ Datasets formatted: {len(formatted_train_dataset):,} training, {len(formatted_validation_dataset):,} validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "## **Step 5: Configure LoRA**\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** allows us to efficiently fine-tune the model by training only a small number of adapter parameters instead of the full model.\n",
    "\n",
    "**Optimized Settings**:\n",
    "- **Rank 16**: Increased from 1 for better model capacity (good balance for fast training)\n",
    "- **Alpha 32**: Typically set to 2√órank for optimal scaling\n",
    "- **Dropout 0.05**: Light regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVZHQ4y74BCG",
    "outputId": "0aeacc2e-f2d0-4dfb-9134-799b59e2f277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA with optimized parameters for fast training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,  # Increased from 1: more capacity for better performance\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 32,  # Typically 2√órank\n",
    "    lora_dropout = 0.05,  # Light regularization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 6: Set Up SFTTrainer**\n",
    "\n",
    "Configure the training process with **Try 1 Fast Training Settings**:\n",
    "- **1 epoch**: Single pass through 45K training dataset for fast completion\n",
    "- **Learning rate 2e-4** (vs 1e-4): Higher LR for faster convergence on smaller dataset ‚¨ÜÔ∏è\n",
    "- **Warmup 100 steps** (vs 500): Shorter warmup to start learning faster ‚¨áÔ∏è\n",
    "- **No validation during training**: For maximum speed\n",
    "- **Save checkpoints at each epoch**: Keep last 2 checkpoints for model recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "60c2fb46114942b18b9ec7d8b3e47796",
      "1995f14190074c95aec830dcfc5e5e35",
      "ad2065bad3bf48baaa84cd4ceefd6094",
      "baa72491fb714d1aa25488fc7efce2d7",
      "57e8f31083a74760b2ad7b43d0a997cf",
      "7cbc243c8c2e453eb34a207f430f96b4",
      "55a5a510add64d5687575d0ef10f5a4f",
      "e91e61fbe6194f3b9f107375502951a9",
      "e6e15dc873c34ca9b67471b8a485912a",
      "ed5e38ae05f1439ebae7a6aff7066b01",
      "0d7917c3b62c49b38f4606b667897f8d"
     ]
    },
    "id": "YVrNhZ4y4zsK",
    "outputId": "ce238a90-93d0-4622-c4b6-ffef5dff862d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c2fb46114942b18b9ec7d8b3e47796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Try 1.5 Trainer configured: 80K samples, LR 2e-4, Warmup 300\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 100,  # Try 1: Reduced from 500 for faster start\n",
    "        num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,  # Try 1: Increased from 1e-4 for faster convergence\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 100,\n",
    "        eval_strategy = \"no\",  # Disable validation during training\n",
    "        save_strategy = \"epoch\",  # Save only at end of each epoch\n",
    "        save_total_limit = 2,  # Keep only 2 checkpoints\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Try 1 Trainer configured: 45K training samples, 1 epoch, LR 2e-4, Warmup 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 7: Start Training**\n",
    "\n",
    "Train the model for **1 epoch** over **45,000 training samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "agvQR_Ku5wWY",
    "outputId": "894b4635-7258-462e-adfb-4c1ab4798666"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 45,000 | Num Epochs = 1 | Total steps = 1,407\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1407' max='1407' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1407/1407 2:19:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.557100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.528200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.480800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.483700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1407, training_loss=0.548470039781138, metrics={'train_runtime': 8401.6724, 'train_samples_per_second': 5.356, 'train_steps_per_second': 0.167, 'total_flos': 9.669392257280901e+17, 'train_loss': 0.548470039781138, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 8: Prepare for Inference**\n",
    "\n",
    "Prepare the trained model for faster inference and test on a validation example to verify it's working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lvcDSh0JZYm",
    "outputId": "d6907d82-d823-4059-8171-212952dae3a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### QUESTION ####\n",
      "A teacher wants to arrange 3 copies of Introduction to Geometry and 4 copies of Introduction to Number Theory on a bookshelf. In how many ways can he do that?\n",
      "\n",
      "#### SOLUTION ####\n",
      "To solve this problem, we can use the binomial coefficient to find the number of ways to arrange the books on the shelf.\n",
      "\n",
      "The binomial coefficient is represented by the formula\n",
      "\\[\\binom{n}{k} = \\frac{...\n",
      "\n",
      "#### STUDENT ANSWER ####\n",
      "\\frac{P!}{O!(P-O)!}\n",
      "\n",
      "#### MODEL'S PREDICTION ####\n",
      "0<|end_of_text|>\n",
      "\n",
      "#### CORRECT ANSWER ####\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Improved inference prompt (matching training prompt structure)\n",
    "inference_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# Test on a validation example\n",
    "example = validation_dataset[1]\n",
    "question = example[\"question\"]\n",
    "solution = example[\"solution\"]\n",
    "answer = example[\"answer\"]\n",
    "\n",
    "# Format the prompt\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt.format(question, str(solution), str(answer))],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate prediction\n",
    "outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "# Display results\n",
    "print(\"#### QUESTION ####\")\n",
    "print(question)\n",
    "print(\"\\n#### SOLUTION ####\")\n",
    "print(solution[:200] + \"...\" if len(str(solution)) > 200 else solution)\n",
    "print(\"\\n#### STUDENT ANSWER ####\")\n",
    "print(answer)\n",
    "print(\"\\n#### MODEL'S PREDICTION ####\")\n",
    "output_part = response.split(\"Output:\\n\")[-1]\n",
    "print(output_part[:50])\n",
    "print(\"\\n#### CORRECT ANSWER ####\")\n",
    "print(example[\"is_correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_608V-_NK3B",
    "outputId": "78db858c-b99e-4b52-e51f-6d0817ea6556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8244\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Improved inference prompt (matching training prompt structure)\n",
    "inference_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "count = 0\n",
    "\n",
    "# Test on a validation example\n",
    "for i in range(len(validation_dataset)):\n",
    "  example = validation_dataset[i]\n",
    "  question = example[\"question\"]\n",
    "  solution = example[\"solution\"]\n",
    "  answer = example[\"answer\"]\n",
    "\n",
    "  # Format the prompt\n",
    "  inputs = tokenizer(\n",
    "      [inference_prompt.format(question, str(solution), str(answer))],\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_seq_length\n",
    "  ).to(\"cuda\")\n",
    "\n",
    "  # Generate prediction\n",
    "  outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "  response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "  if example[\"is_correct\"] == int(response.split(\"Output:\\n\")[-1].split(\"<\")[0]):\n",
    "    count += 1\n",
    "\n",
    "print(f\"Accuracy: {count/len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Sau2baFPJ7h",
    "outputId": "274ddb35-68a7-4f04-e8ec-278046005b07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 23842.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data saved to 'test_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "\n",
    "questions = []\n",
    "solutions = []\n",
    "answers = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    questions.append(question)\n",
    "    solutions.append(solution)\n",
    "    answers.append(answer)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Question': questions,\n",
    "    'Solution': solutions,\n",
    "    'Answer': answers\n",
    "})\n",
    "\n",
    "df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(f\"Test data saved to 'test_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5489c0ed"
   },
   "source": [
    "## **Step 9: Generate Submission File**\n",
    "\n",
    "Generate predictions for all test samples and create the submission CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e020e6b",
    "outputId": "b735a2b9-75c1-49e4-f6c8-d8efdb487827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 10,000 test samples...\n",
      "Using constrained decoding with allowed tokens: ['True', 'False', '1', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [32:26<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Submission file created successfully!\n",
      "   Total predictions: 10,000\n",
      "   True predictions: 3,608\n",
      "   False predictions: 6,392\n",
      "\n",
      "üìÅ File saved as 'submission.csv'\n",
      "   You can now download this file and submit it to the Kaggle competition.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Constrained Decoding: Force model to generate only \"True\"/\"False\" or \"1\"/\"0\" tokens\n",
    "class AllowedTokensLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"Logits Processor that forces model to generate only allowed tokens\"\"\"\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed = set(allowed_token_ids)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Set probability of disallowed tokens to -inf\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for tid in self.allowed:\n",
    "            if tid < scores.shape[-1]:\n",
    "                mask[..., tid] = 0.0\n",
    "        return scores + mask\n",
    "\n",
    "def get_allowed_token_ids(tokenizer):\n",
    "    \"\"\"Return token IDs for allowed tokens (True/False or 1/0)\"\"\"\n",
    "    tokens = [\"True\", \"False\", \"1\", \"0\"]\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id != tokenizer.unk_token_id:\n",
    "            ids.append(token_id)\n",
    "    return ids\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "predictions = []\n",
    "\n",
    "# Setup constrained decoding\n",
    "allowed_ids = get_allowed_token_ids(tokenizer)\n",
    "logits_proc = AllowedTokensLogitsProcessor(allowed_ids)\n",
    "\n",
    "# Optimized Generation configuration\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=1,        # Reduced from 8 to 1 (faster and more accurate)\n",
    "    do_sample=False,          # Deterministic generation\n",
    "    temperature=0.0,          # Use probability distribution as-is\n",
    "    top_p=1.0,\n",
    "    eos_token_id=[tokenizer.eos_token_id],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Generate predictions for all test samples\n",
    "print(f\"Generating predictions for {len(test_dataset):,} test samples...\")\n",
    "allowed_tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in allowed_ids if tid != tokenizer.unk_token_id]\n",
    "print(f\"Using constrained decoding with allowed tokens: {allowed_tokens}\")\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]  # Student answer\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = inference_prompt.format(question, str(solution), str(answer))\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate prediction with constrained decoding\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        logits_processor=[logits_proc],  # Apply constrained decoding\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    # Decode only newly generated tokens (more accurate parsing)\n",
    "    new_tokens = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Check first character only (0 or 1, True/False)\n",
    "    ch = response_text.strip()[:1] if response_text.strip() else \"0\"\n",
    "\n",
    "    # Parse: \"1\" or first letter \"T\" of \"True\" means True\n",
    "    if ch.lower() in [\"1\", \"t\"]:\n",
    "        prediction = True\n",
    "    else:\n",
    "        prediction = False\n",
    "\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission file created successfully!\")\n",
    "print(f\"   Total predictions: {len(predictions):,}\")\n",
    "print(f\"   True predictions: {sum(predictions):,}\")\n",
    "print(f\"   False predictions: {len(predictions) - sum(predictions):,}\")\n",
    "print(\"\\nüìÅ File saved as 'submission.csv'\")\n",
    "print(\"   You can now download this file and submit it to the Kaggle competition.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}