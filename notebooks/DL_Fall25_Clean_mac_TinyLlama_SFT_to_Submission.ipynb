{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff9771e",
   "metadata": {},
   "source": [
    "\n",
    "# DL-Fall-25 â€” Clean Notebook (macOS-friendly)\n",
    "**TinyLlama SFT (LoRA, no bitsandbytes) â†’ submission.csv**\n",
    "\n",
    "Runs locally on macOS (no CUDA). Trains a small open model (TinyLlama 1.1B) with PEFT LoRA and writes a Kaggle-ready `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab58fc",
   "metadata": {},
   "source": [
    "## 0) Environment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62a4347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.13.3\n",
      "OS     : macOS-15.6.1-arm64-arm-64bit-Mach-O\n",
      "CUDA   : None\n",
      "GPU    : NO CUDA GPU\n",
      "MPS    : True\n",
      "Executable: /opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import platform, torch, sys\n",
    "print(\"Python :\", platform.python_version())\n",
    "print(\"OS     :\", platform.platform())\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"GPU    :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"NO CUDA GPU\")\n",
    "print(\"MPS    :\", getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available())\n",
    "print(\"Executable:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5434f",
   "metadata": {},
   "source": [
    "## 1) Clean & install deps (no SciPy/sklearn; pinned transformers/TRl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef1e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping scipy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[99 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Running `maturin pep517 build-wheel -i /opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python --compatibility off`\n",
      "  \u001b[31m   \u001b[0m Python reports SOABI: cpython-313-darwin\n",
      "  \u001b[31m   \u001b[0m Computed rustc target triple: aarch64-apple-darwin\n",
      "  \u001b[31m   \u001b[0m Installation directory: /Users/yuktakulkarni/Library/Caches/puccinialin\n",
      "  \u001b[31m   \u001b[0m Rustup already downloaded\n",
      "  \u001b[31m   \u001b[0m Installing rust to /Users/yuktakulkarni/Library/Caches/puccinialin/rustup\n",
      "  \u001b[31m   \u001b[0m warn: It looks like you have an existing rustup settings file at:\n",
      "  \u001b[31m   \u001b[0m warn: /Users/yuktakulkarni/.rustup/settings.toml\n",
      "  \u001b[31m   \u001b[0m warn: Rustup will install the default toolchain as specified in the settings file,\n",
      "  \u001b[31m   \u001b[0m warn: instead of the one inferred from the default host triple.\n",
      "  \u001b[31m   \u001b[0m info: profile set to 'minimal'\n",
      "  \u001b[31m   \u001b[0m info: default host triple is aarch64-apple-darwin\n",
      "  \u001b[31m   \u001b[0m warn: Updating existing toolchain, profile choice will be ignored\n",
      "  \u001b[31m   \u001b[0m info: syncing channel updates for 'stable-aarch64-apple-darwin'\n",
      "  \u001b[31m   \u001b[0m info: default toolchain set to 'stable-aarch64-apple-darwin'\n",
      "  \u001b[31m   \u001b[0m Checking if cargo is installed\n",
      "  \u001b[31m   \u001b[0m cargo 1.90.0 (840b83a10 2025-07-30)\n",
      "  \u001b[31m   \u001b[0m âš ï¸  Warning: `project.version` field is required in pyproject.toml unless it is present in the `project.dynamic` list\n",
      "  \u001b[31m   \u001b[0m ðŸ¹ Building a mixed python/rust project\n",
      "  \u001b[31m   \u001b[0m ðŸ”— Found pyo3 bindings\n",
      "  \u001b[31m   \u001b[0m ðŸ Found CPython 3.13 at /opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python\n",
      "  \u001b[31m   \u001b[0m ðŸ“¡ Using build options features, bindings from pyproject.toml\n",
      "  \u001b[31m   \u001b[0m ðŸ’» Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by default\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.81\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.12\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.2.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m target-lexicon v0.12.14\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.153\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.19.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m cfg-if v1.0.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-utils v0.8.19\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m cc v1.0.94\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m memchr v2.7.2\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m ident_case v1.0.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m strsim v0.10.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m fnv v1.0.7\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m portable-atomic v1.6.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m smallvec v1.13.2\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m serde v1.0.198\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m num-traits v0.2.18\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m lock_api v0.4.11\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m parking_lot_core v0.9.9\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m aho-corasick v1.1.3\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pkg-config v0.3.30\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-epoch v0.9.18\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m rayon-core v1.12.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.36\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m regex-syntax v0.8.3\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m syn v2.0.60\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m either v1.11.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m onig_sys v69.8.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m getrandom v0.2.14\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam-deque v0.8.5\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-build-config v0.21.2\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m memoffset v0.9.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m matrixmultiply v0.3.8\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m scopeguard v1.2.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m paste v1.0.14\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.4.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m rand_core v0.6.4\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m esaxx-rs v0.1.10\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m rawpointer v0.2.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m log v0.4.21\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m unicode-width v0.1.11\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m serde_json v1.0.116\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m thiserror v1.0.58\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m minimal-lexical v0.2.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m lazy_static v1.4.0\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m utf8parse v0.2.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m ppv-lite86 v0.2.17\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m console v0.15.8\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m anstyle-parse v0.2.3\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m nom v7.1.3\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m rand_chacha v0.3.1\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m regex-automata v0.4.6\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m pyo3-ffi v0.21.2\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[32m   Compiling\u001b[0m darling_core v0.20.8\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[31merror\u001b[0m\u001b[1m:\u001b[0m failed to run custom build command for `pyo3-ffi v0.21.2`\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Caused by:\n",
      "  \u001b[31m   \u001b[0m   process didn't exit successfully: `/private/var/folders/39/h509vzss5cb4ftjj6z648wsm0000gn/T/pip-install-foe8_v64/tokenizers_61ce794c68d44b6ea1c5aa8768cc617f/bindings/python/target/release/build/pyo3-ffi-d50a844054f25cb3/build-script-build` (exit status: 1)\n",
      "  \u001b[31m   \u001b[0m   --- stdout\n",
      "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS\n",
      "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS_LIB_DIR\n",
      "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS_PYTHON_VERSION\n",
      "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_CROSS_PYTHON_IMPLEMENTATION\n",
      "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_PRINT_CONFIG\n",
      "  \u001b[31m   \u001b[0m   cargo:rerun-if-env-changed=PYO3_USE_ABI3_FORWARD_COMPATIBILITY\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   --- stderr\n",
      "  \u001b[31m   \u001b[0m   error: the configured Python interpreter version (3.13) is newer than PyO3's maximum supported version (3.12)\n",
      "  \u001b[31m   \u001b[0m   = help: please check if an updated version of PyO3 is available. Current version: 0.21.2\n",
      "  \u001b[31m   \u001b[0m   = help: set PYO3_USE_ABI3_FORWARD_COMPATIBILITY=1 to suppress this check and build anyway using the stable ABI\n",
      "  \u001b[31m   \u001b[0m \u001b[1m\u001b[33mwarning\u001b[0m\u001b[1m:\u001b[0m build failed, waiting for other jobs to finish...\n",
      "  \u001b[31m   \u001b[0m ðŸ’¥ maturin failed\n",
      "  \u001b[31m   \u001b[0m   Caused by: Failed to build a native library through cargo\n",
      "  \u001b[31m   \u001b[0m   Caused by: Cargo build finished with \"exit status: 101\": `env -u CARGO MACOSX_DEPLOYMENT_TARGET=\"11.0\" PYO3_BUILD_EXTENSION_MODULE=\"1\" PYO3_ENVIRONMENT_SIGNATURE=\"cpython-3.13-64bit\" PYO3_PYTHON=\"/opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python\" PYTHON_SYS_EXECUTABLE=\"/opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python\" \"cargo\" \"rustc\" \"--features\" \"pyo3/extension-module\" \"--message-format\" \"json-render-diagnostics\" \"--manifest-path\" \"/private/var/folders/39/h509vzss5cb4ftjj6z648wsm0000gn/T/pip-install-foe8_v64/tokenizers_61ce794c68d44b6ea1c5aa8768cc617f/bindings/python/Cargo.toml\" \"--release\" \"--lib\" \"--\" \"-C\" \"link-arg=-undefined\" \"-C\" \"link-arg=dynamic_lookup\" \"-C\" \"link-args=-Wl,-install_name,@rpath/tokenizers.tokenizers.cpython-313-darwin.so\"`\n",
      "  \u001b[31m   \u001b[0m Rust not found, installing into a temporary directory\n",
      "  \u001b[31m   \u001b[0m Error: command ['maturin', 'pep517', 'build-wheel', '-i', '/opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python', '--compatibility', 'off'] returned non-zero exit status 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Deps installed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip -q uninstall -y scipy scikit-learn\n",
    "%pip -q install -U \"numpy>=1.26,<3.0\" \"pandas>=2.1.0\" \"datasets>=2.20.0\" \"accelerate>=0.33.0\" \"transformers==4.40.2\" \"trl==0.9.6\" \"peft>=0.12.0\" \"evaluate>=0.4.2\" \"torch>=2.3.0\"\n",
    "print(\"âœ… Deps installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf6dc5",
   "metadata": {},
   "source": [
    "## 2) Import libs (block optional sklearn path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ffa992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Skipping tf-keras as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-macos as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-metal as it is not installed.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Disabled TF/Flax and cleared related modules.\n"
     ]
    }
   ],
   "source": [
    "# --- Fix: disable TF/Flax in Transformers and purge TF/Keras from this kernel ---\n",
    "%pip -q uninstall -y keras tf-keras tensorflow tensorflow-macos tensorflow-metal\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "# If parts of these libs were already imported, drop them so re-imports use new flags\n",
    "for m in list(sys.modules):\n",
    "    if m.startswith((\"keras\", \"tensorflow\", \"transformers\")):\n",
    "        sys.modules.pop(m, None)\n",
    "\n",
    "print(\"âœ… Disabled TF/Flax and cleared related modules.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27be3292-2530-42d0-ba52-812c3b2bf107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.3.4_1/libexec/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports OK\n"
     ]
    }
   ],
   "source": [
    "import transformers.utils.import_utils as iu\n",
    "iu.is_sklearn_available = lambda: False  # keep sklearn out\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, LogitsProcessor\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import os, random, numpy as np, torch, pandas as pd\n",
    "print(\"âœ… Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea9978f",
   "metadata": {},
   "source": [
    "## 3) Load dataset (public HF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a023395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'is_correct', 'answer', 'solution'],\n",
      "    num_rows: 1000000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'is_correct', 'answer', 'solution'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
    "train_ds, test_ds = ds[\"train\"], ds[\"test\"]\n",
    "print(train_ds); print(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698c9e3",
   "metadata": {},
   "source": [
    "## (Optional) Local CSV loader (uncomment and set paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ca889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TRAIN_CSV = \"train.csv\"; TEST_CSV  = \"test.csv\"\n",
    "# import pandas as pd\n",
    "# df_train = pd.read_csv(TRAIN_CSV); df_test  = pd.read_csv(TEST_CSV)\n",
    "# df_train[\"is_correct\"] = (df_train[\"is_correct\"].astype(str).str.strip().str.lower()\n",
    "#                           .map({\"true\": True, \"false\": False, \"1\": True, \"0\": False}))\n",
    "# from datasets import Dataset\n",
    "# train_ds = Dataset.from_pandas(df_train, preserve_index=False)\n",
    "# test_ds  = Dataset.from_pandas(df_test,  preserve_index=False)\n",
    "# print(train_ds); print(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2bbcb",
   "metadata": {},
   "source": [
    "## 4) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b3060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0', out_dir='outputs', use_solution=1, epochs=1, lr=0.0002, per_device_bs=1, grad_accum=8, max_len=1024, seed=42, report_to='none', TRAIN_LIMIT=5000, VAL_LIMIT=500, TEST_LIMIT=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_id: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    out_dir: str = \"outputs\"\n",
    "    use_solution: int = 1\n",
    "    epochs: int = 1\n",
    "    lr: float = 2e-4\n",
    "    per_device_bs: int = 1\n",
    "    grad_accum: int = 8\n",
    "    max_len: int = 1024\n",
    "    seed: int = 42\n",
    "    report_to: str = \"none\"\n",
    "    TRAIN_LIMIT: int | None = 5000\n",
    "    VAL_LIMIT:   int | None = 500\n",
    "    TEST_LIMIT:  int | None = None\n",
    "cfg = Config(); cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d391a3",
   "metadata": {},
   "source": [
    "## 5) Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093eeb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Utils ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "from transformers import LogitsProcessor\n",
    "import torch, numpy as np, random\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are a precise math answer verifier. \"\n",
    "    \"Given a problem, a student's answer, and an optional worked solution, \"\n",
    "    \"respond with a single digit label without explanation:\\n\"\n",
    "    \"- 1 if the student's answer is correct\\n\"\n",
    "    \"- 0 if the student's answer is incorrect\"\n",
    ")\n",
    "\n",
    "def build_user_block(example, use_solution: bool = True) -> str:\n",
    "    q = (example.get(\"question\") or \"\").strip()\n",
    "    a = (example.get(\"answer\") or \"\").strip()\n",
    "    sol = example.get(\"solution\") or \"\"\n",
    "    parts = [f\"Problem:\\n{q}\\n\\nStudent Answer:\\n{a}\"]\n",
    "    if use_solution and isinstance(sol, str) and sol.strip():\n",
    "        parts.append(f\"\\nSolution (reference reasoning):\\n{sol.strip()}\")\n",
    "    parts.append(\"\\nReturn only the digit (1 or 0).\")\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def make_text(example, use_solution: bool, label=None):\n",
    "    hdr = f\"<<SYS>>\\n{SYS_PROMPT}\\n<</SYS>>\\n\\n\"\n",
    "    usr = f\"<<USR>>\\n{build_user_block(example, use_solution)}\\n<</USR>>\\n\\n\"\n",
    "    asst = \"<<ASSISTANT>>\\n\" + (str(int(label)) if label is not None else \"\")\n",
    "    return hdr + usr + asst\n",
    "\n",
    "class AllowedTokensLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_token_ids): self.allowed = set(allowed_token_ids)\n",
    "    def __call__(self, input_ids, scores):\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for tid in self.allowed:\n",
    "            if tid < scores.shape[-1]:\n",
    "                mask[..., tid] = 0.0\n",
    "        return scores + mask\n",
    "\n",
    "def get_allowed_token_ids(tokenizer):\n",
    "    return [tokenizer.convert_tokens_to_ids(t) for t in [\"0\", \"1\"]]\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"âœ… Utils ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46baa6",
   "metadata": {},
   "source": [
    "## 6) Split (ClassLabel + subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6705bd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_ds = train_ds.class_encode_column(\"is_correct\")\n",
    "dd = train_ds.train_test_split(test_size=0.05, seed=42, stratify_by_column=\"is_correct\")\n",
    "if cfg.TRAIN_LIMIT:\n",
    "    dd[\"train\"] = dd[\"train\"].shuffle(seed=42).select(range(cfg.TRAIN_LIMIT))\n",
    "if cfg.VAL_LIMIT:\n",
    "    dd[\"test\"]  = dd[\"test\"].shuffle(seed=42).select(range(cfg.VAL_LIMIT))\n",
    "dataset = DatasetDict({\"train\": dd[\"train\"], \"validation\": dd[\"test\"]})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7683b",
   "metadata": {},
   "source": [
    "## 7) Tokenizer + model (MPS if available) + LoRA + tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96bb5636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id, use_fast=True)  # <- fast tokenizer, no sentencepiece needed\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "\n",
    "\n",
    "import torch\n",
    "device_map = \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"auto\"\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.model_id, device_map=device_map)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "peft_cfg = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def preprocess(example, use_solution: bool):\n",
    "    label = 1 if example[\"is_correct\"] else 0\n",
    "    return {\"text\": make_text(example, use_solution=bool(use_solution), label=label)}\n",
    "\n",
    "proc_train = dataset[\"train\"].map(lambda x: preprocess(x, True))\n",
    "proc_val   = dataset[\"validation\"].map(lambda x: preprocess(x, True))\n",
    "len(proc_train), len(proc_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1415090",
   "metadata": {},
   "source": [
    "## 8) Train (TRL SFT) + save adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8329bd74-c3fa-4c2a-8d69-ebf458310e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 02:47, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.788600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fast training done. Saved LoRA adapter to: outputs_fast/adapter\n"
     ]
    }
   ],
   "source": [
    "# ==== FAST TRAIN (subset + few steps) ====\n",
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "# use small subset\n",
    "N_TRAIN = 600     # reduce to taste\n",
    "N_VAL   = 0       # skip val to save time (set 100 if you want a tiny val)\n",
    "MAX_LEN = 384     # shorter sequences are faster\n",
    "MAX_STEPS = 120   # hard cap; overrides epochs if > 0\n",
    "\n",
    "# build subset from your *preprocessed* data\n",
    "small_train = proc_train.select(range(min(N_TRAIN, len(proc_train))))\n",
    "small_val   = proc_val.select(range(min(N_VAL,   len(proc_val))))\n",
    "\n",
    "def tok_fast(batch):\n",
    "    out = tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tok_train_fast = small_train.map(tok_fast, batched=True, remove_columns=small_train.column_names)\n",
    "tok_val_fast   = small_val.map(tok_fast,   batched=True, remove_columns=small_val.column_names) if N_VAL>0 else None\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_args_fast = TrainingArguments(\n",
    "    output_dir=\"outputs_fast\",\n",
    "    num_train_epochs=0.5,              # ignored if MAX_STEPS>0\n",
    "    max_steps=MAX_STEPS,               # keep small\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=cfg.lr,\n",
    "    logging_steps=50,\n",
    "    save_steps=10_000_000,             # effectively disable saves during train\n",
    "    save_total_limit=1,\n",
    "    fp16=False, bf16=False,\n",
    "    dataloader_pin_memory=False,       # silence MPS warning\n",
    "    report_to=[],                      # no logging backends\n",
    ")\n",
    "\n",
    "trainer_fast = Trainer(\n",
    "    model=model,                       # your LoRA-wrapped model\n",
    "    args=train_args_fast,\n",
    "    train_dataset=tok_train_fast,\n",
    "    eval_dataset=tok_val_fast,         # None if N_VAL=0\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer_fast.train()\n",
    "\n",
    "# save adapter separately\n",
    "import os\n",
    "adapter_fast_dir = os.path.join(\"outputs_fast\", \"adapter\")\n",
    "model.save_pretrained(adapter_fast_dir)\n",
    "tokenizer.save_pretrained(\"outputs_fast\")\n",
    "print(f\"âœ… Fast training done. Saved LoRA adapter to: {adapter_fast_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24e05a",
   "metadata": {},
   "source": [
    "## 9) Inference â†’ `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba13cac-e758-46ea-9815-d6ee9a1dae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/1000\n",
      "404/1000\n",
      "804/1000\n",
      "Saved submission to: outputs_fast/submission.csv  (rows=1000)\n"
     ]
    }
   ],
   "source": [
    "# ===== Context-aware no-generate inference (CPU, left-pad) =====\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch, os, pandas as pd\n",
    "\n",
    "# knobs\n",
    "TEST_LIMIT    = 1000      # set to e.g. 1000 for quick test; None = full 10k\n",
    "BATCH_SIZE    = 4         # safe; bump to 8 if smooth\n",
    "MAX_LEN_INFER = 192\n",
    "\n",
    "# tokenizer settings for decoder-only models\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 4) - 1))\n",
    "\n",
    "# reload base + LoRA on CPU\n",
    "base = AutoModelForCausalLM.from_pretrained(cfg.model_id, dtype=torch.float32, low_cpu_mem_usage=True)\n",
    "model_inf = PeftModel.from_pretrained(base, os.path.join(\"outputs_fast\", \"adapter\"))\n",
    "model_inf.to(DEVICE).eval()\n",
    "\n",
    "def next_token_id_in_context(prompt: str, digit: str) -> int:\n",
    "    \"\"\"\n",
    "    Get the FIRST token id the tokenizer would emit for continuing `prompt` with `digit`\n",
    "    under the SAME truncation/left-padding regime used in inference.\n",
    "    \"\"\"\n",
    "    ids_base = tokenizer(prompt, add_special_tokens=False, truncation=True,\n",
    "                         max_length=MAX_LEN_INFER)[\"input_ids\"]\n",
    "    ids_plus = tokenizer(prompt + digit, add_special_tokens=False, truncation=True,\n",
    "                         max_length=MAX_LEN_INFER)[\"input_ids\"]\n",
    "    # With left truncation, we can't rely on lengths lining up; take last id as the continuation token.\n",
    "    if not ids_plus:\n",
    "        return None\n",
    "    return ids_plus[-1]\n",
    "\n",
    "# choose test slice\n",
    "ds = test_ds if TEST_LIMIT is None else test_ds.select(range(min(TEST_LIMIT, len(test_ds))))\n",
    "\n",
    "preds = []\n",
    "N = len(ds)\n",
    "for start in range(0, N, BATCH_SIZE):\n",
    "    end = min(start + BATCH_SIZE, N)\n",
    "    prompts = [make_text(ds[i], use_solution=bool(cfg.use_solution), label=None) for i in range(start, end)]\n",
    "\n",
    "    # context-aware ids for each row\n",
    "    id0_list = [next_token_id_in_context(p, \"0\") for p in prompts]\n",
    "    id1_list = [next_token_id_in_context(p, \"1\") for p in prompts]\n",
    "\n",
    "    # encode prompts\n",
    "    enc = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN_INFER)\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model_inf(**enc)             # [B, L, V]\n",
    "        logits = out.logits\n",
    "        last_idx = enc[\"attention_mask\"].sum(dim=1) - 1\n",
    "        B = logits.size(0)\n",
    "        last = logits[torch.arange(B, device=DEVICE), last_idx]   # [B, V]\n",
    "\n",
    "        batch_preds = []\n",
    "        for b in range(B):\n",
    "            t0 = id0_list[b]; t1 = id1_list[b]\n",
    "            # if any id is None, fall back to old simple ids\n",
    "            if t0 is None or t1 is None:\n",
    "                # robust fallback: try raw digit id, space+digit, newline+digit\n",
    "                cand = []\n",
    "                for s in (\"0\", \" 0\", \"\\n0\"):\n",
    "                    tok = tokenizer.encode(s, add_special_tokens=False)\n",
    "                    if tok: cand.append(tok[-1])\n",
    "                t0 = (cand[-1] if cand else None)\n",
    "\n",
    "                cand = []\n",
    "                for s in (\"1\", \" 1\", \"\\n1\"):\n",
    "                    tok = tokenizer.encode(s, add_special_tokens=False)\n",
    "                    if tok: cand.append(tok[-1])\n",
    "                t1 = (cand[-1] if cand else None)\n",
    "\n",
    "            # if still missing, default to False\n",
    "            if t0 is None or t1 is None or t0 >= last.shape[-1] or t1 >= last.shape[-1]:\n",
    "                batch_preds.append(False)\n",
    "                continue\n",
    "\n",
    "            batch_preds.append(last[b, t1].item() > last[b, t0].item())\n",
    "\n",
    "    preds.extend({\"ID\": i, \"is_correct\": bool(p)} for i, p in zip(range(start, end), batch_preds))\n",
    "    if (start // BATCH_SIZE) % 100 == 0:\n",
    "        print(f\"{end}/{N}\")\n",
    "\n",
    "# save CSV\n",
    "os.makedirs(\"outputs_fast\", exist_ok=True)\n",
    "sub_path = os.path.join(\"outputs_fast\", \"submission.csv\")\n",
    "pd.DataFrame(preds, columns=[\"ID\",\"is_correct\"]).to_csv(sub_path, index=False)\n",
    "print(f\"Saved submission to: {sub_path}  (rows={len(preds)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b4540",
   "metadata": {},
   "source": [
    "## 10) Validate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e44f823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  is_correct\n",
       "0   0       False\n",
       "1   1       False\n",
       "2   2       False\n",
       "3   3       False\n",
       "4   4       False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1000 Unique IDs: 1000\n",
      "âœ… Submission looks good at outputs_fast/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "p = os.path.join(\"outputs_fast\", \"submission.csv\")\n",
    "df = pd.read_csv(p)\n",
    "display(df.head())\n",
    "print(\"Rows:\", len(df), \"Unique IDs:\", df[\"ID\"].nunique())\n",
    "assert set(df[\"is_correct\"].unique()).issubset({True, False})\n",
    "print(\"âœ… Submission looks good at\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f3a13",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Kaggle GPU quick swap (Llama-3-8B + QLoRA)\n",
    "1) Settings: GPU **T4**, Internet **On**; add Secret **HF_TOKEN**.  \n",
    "2) Install:\n",
    "```\n",
    "%pip -q install -U \"accelerate>=0.33.0\" \"bitsandbytes>=0.43.0\" \"datasets>=2.20.0\" \"evaluate>=0.4.2\" \"numpy>=1.26.0\" \"pandas>=2.1.0\" \"peft>=0.12.0\" \"scikit-learn>=1.3.0\" \"transformers>=4.44.0\" \"trl>=0.10.1\"\n",
    "```\n",
    "3) Set:\n",
    "```\n",
    "cfg.model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "cfg.per_device_bs, cfg.grad_accum, cfg.max_len = 1, 16, 2048\n",
    "from huggingface_hub import login; import os; login(os.environ[\"HF_TOKEN\"])\n",
    "```\n",
    "4) Re-run from **dataset** onward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec375ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig_trainer\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m     57\u001b[0m     trainer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mmax_len\n\u001b[0;32m---> 59\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrainer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     62\u001b[0m adapter_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39mout_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madapter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SFTTrainer.__init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# Robust SFTConfig + SFTTrainer setup across TRL versions\n",
    "from inspect import signature\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os\n",
    "\n",
    "# ---------- 1) Build SFTConfig with only stable keys, and add eval/log if supported ----------\n",
    "cfg_kwargs = dict(\n",
    "    output_dir=cfg.out_dir,\n",
    "    num_train_epochs=cfg.epochs,\n",
    "    learning_rate=cfg.lr,\n",
    "    per_device_train_batch_size=cfg.per_device_bs,\n",
    "    per_device_eval_batch_size=max(1, cfg.per_device_bs//2),\n",
    "    gradient_accumulation_steps=cfg.grad_accum,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    bf16=False, fp16=False, tf32=False,     # safe on CPU/MPS\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=cfg.report_to,\n",
    ")\n",
    "\n",
    "sig_cfg = signature(SFTConfig)\n",
    "# evaluation arg changed across versions; add the one your TRL exposes\n",
    "if \"evaluation_strategy\" in sig_cfg.parameters:\n",
    "    cfg_kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "elif \"eval_strategy\" in sig_cfg.parameters:\n",
    "    cfg_kwargs[\"eval_strategy\"] = \"steps\"\n",
    "# Some TRL versions accept max_length here; others want it on the trainer. We'll prefer trainer side.\n",
    "if \"max_length\" in sig_cfg.parameters:\n",
    "    cfg_kwargs[\"max_length\"] = cfg.max_len\n",
    "# (avoid max_seq_length here; weâ€™ll set it on the trainer where itâ€™s more stable)\n",
    "\n",
    "sft_config = SFTConfig(**cfg_kwargs)\n",
    "\n",
    "# ---------- 2) Build SFTTrainer with dataset-specific args passed here ----------\n",
    "trainer_kwargs = dict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=proc_train,\n",
    "    eval_dataset=proc_val,   # harmless if eval strategy is 'no'\n",
    ")\n",
    "\n",
    "sig_trainer = signature(SFTTrainer.__init__)\n",
    "# Pass dataset_text_field/packing if this TRL exposes them on the trainer\n",
    "if \"dataset_text_field\" in sig_trainer.parameters:\n",
    "    trainer_kwargs[\"dataset_text_field\"] = \"text\"\n",
    "if \"packing\" in sig_trainer.parameters:\n",
    "    trainer_kwargs[\"packing\"] = True\n",
    "\n",
    "# max length key varies: some versions use max_seq_length, others max_length on trainer\n",
    "if \"max_seq_length\" in sig_trainer.parameters:\n",
    "    trainer_kwargs[\"max_seq_length\"] = cfg.max_len\n",
    "elif \"max_length\" in sig_trainer.parameters:\n",
    "    trainer_kwargs[\"max_length\"] = cfg.max_len\n",
    "\n",
    "trainer = SFTTrainer(**trainer_kwargs)\n",
    "trainer.train()\n",
    "\n",
    "adapter_dir = os.path.join(cfg.out_dir, \"adapter\")\n",
    "trainer.model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(cfg.out_dir)\n",
    "print(f\"âœ… Trained. Saved LoRA adapter to: {adapter_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from peft import PeftModel\n",
    "base = AutoModelForCausalLM.from_pretrained(cfg.model_id, device_map=device_map)\n",
    "model_inf = PeftModel.from_pretrained(base, os.path.join(cfg.out_dir, \"adapter\")); model_inf.eval()\n",
    "\n",
    "allowed_ids = get_allowed_token_ids(tokenizer)\n",
    "logits_proc = AllowedTokensLogitsProcessor(allowed_ids)\n",
    "gen_cfg = GenerationConfig(max_new_tokens=1, do_sample=False, temperature=0.0, top_p=1.0,\n",
    "                           eos_token_id=[tokenizer.eos_token_id], pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "test_for_pred = test_ds if cfg.TEST_LIMIT is None else test_ds.select(range(cfg.TEST_LIMIT))\n",
    "preds = []\n",
    "for i, ex in enumerate(test_for_pred):\n",
    "    prompt = make_text(ex, use_solution=True, label=None)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=cfg.max_len)\n",
    "    inputs = {k: v.to(model_inf.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model_inf.generate(**inputs, generation_config=gen_cfg, logits_processor=[logits_proc])\n",
    "    new_tokens = out[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    ch = (text.strip()[:1] if text.strip() else \"0\")\n",
    "    preds.append({\"ID\": i, \"is_correct\": (ch == \"1\")})\n",
    "\n",
    "import pandas as pd, os\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "sub_path = os.path.join(cfg.out_dir, \"submission.csv\")\n",
    "pd.DataFrame(preds, columns=[\"ID\",\"is_correct\"]).to_csv(sub_path, index=False)\n",
    "print(f\"Saved submission to: {sub_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "349e966d-6c63-4b7d-b563-4facaa2b4bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook working dir: /Users/yuktakulkarni/DL_Midterm\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "total 1056\n",
      "drwxr-xr-x@   9 yuktakulkarni  staff     288 Oct 26 19:37 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-x---+ 132 yuktakulkarni  staff    4224 Oct 26 19:37 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x@   6 yuktakulkarni  staff     192 Oct 26 19:37 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 yuktakulkarni  staff   46338 Oct 26 12:06 DL_Fall25_Clean_mac_TinyLlama_SFT_to_Submission.ipynb\n",
      "-rw-r--r--@   1 yuktakulkarni  staff  107081 Oct 26 00:07 DL_Fall25_Llama3_SFT_QLoRA_EndToEnd_v3_mac_or_kaggle.ipynb\n",
      "drwxr-xr-x@   2 yuktakulkarni  staff      64 Oct 25 15:37 \u001b[34moutputs\u001b[m\u001b[m\n",
      "drwxr-xr-x@  12 yuktakulkarni  staff     384 Oct 26 11:41 \u001b[34moutputs_fast\u001b[m\u001b[m\n",
      "-rw-r--r--@   1 yuktakulkarni  staff  333904 Oct 25 15:04 sample_sub.csv\n",
      "-rw-r--r--@   1 yuktakulkarni  staff   44690 Oct 26 00:12 Untitled.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(\"Notebook working dir:\", os.getcwd())\n",
    "!ls -la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "260c65f6-90d0-4ba9-a03e-a47bf5179d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ready\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "try:\n",
    "    get_ipython().run_line_magic(\"config\", \"HistoryManager.enabled = False\")\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"âœ… Ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85a255-2263-4f4b-b64e-a864f889ef6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
