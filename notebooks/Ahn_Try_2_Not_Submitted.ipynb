{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "# Math Question Answer Verification Competition\n",
    "\n",
    "**Goal**: Fine-tune Llama-3-8B model to predict if a given solution to a math problem is correct or not.\n",
    "\n",
    "**Try 2 Configuration (90K Samples)**:\n",
    "- **81,000 training samples** + **9,000 validation samples**\n",
    "- **LoRA rank 20** for good capacity\n",
    "- **LoRA alpha 40** (2√órank for optimal scaling)\n",
    "- **LoRA dropout 0.1** for regularization\n",
    "- **Max sequence length 2048** to prevent truncation\n",
    "- **1 epoch** training (fast completion)\n",
    "- **Learning rate 1e-4** for stable convergence\n",
    "- **Warmup 100 steps** for quick start\n",
    "- **Batch size 8, Gradient accumulation 8** (effective batch size 64)\n",
    "- **Improved prompt template** emphasizing solution reasoning\n",
    "- **Constrained decoding** for reliable output parsing\n",
    "\n",
    "**Note**: This configuration was tested but **not submitted** to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xStnwtpOqK0e"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    import torch; v = re.match(r\"[0-9.]{3,}\", torch.__version__).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Hugging Face Login**\n",
    "\n",
    "For accessing gated models like Llama-3-8B, we need to authenticate with Hugging Face using your token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URSw7qlhqlgB",
    "outputId": "c9a65746-a75a-4022-c675-51c1b4cb29d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Warning: HF_TOKEN not found. Make sure to set it in Colab Secrets if the model is gated.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face using token from Colab Secrets\n",
    "# Set HF_TOKEN in Colab Secrets (Secrets ‚Üí Add Secret)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    login(token=hf_token, add_to_git_credential=False)\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Warning: HF_TOKEN not found. Make sure to set it in Colab Secrets if the model is gated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Load the Model and Tokenizer**\n",
    "\n",
    "Load Llama-3-8B using Unsloth's FastLanguageModel with optimized settings:\n",
    "- **4-bit quantization**: Reduces GPU memory usage significantly\n",
    "- **Max sequence length 2048**: Prevents truncation of long problems\n",
    "- **Auto dtype detection**: Optimizes for your GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etaDwWGN3X7C",
    "outputId": "4a74bb10-270e-474f-cb2e-08b532663ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.12: Fast Llama patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "‚úÖ Model loaded with max_seq_length=2048\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Maximum sequence length to prevent truncation\n",
    "dtype = None  # Auto-detect best data type for GPU\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "# Using Meta-Llama-3.1-8B\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded with max_seq_length={max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5cL3djv3bRy",
    "outputId": "7c2d1a19-7f54-4de6-df74-b5b6df1f3c34"
   },
   "source": [
    "## **Step 4: Prepare the Dataset**\n",
    "\n",
    "This step prepares the training data for fine-tuning. It consists of three parts:\n",
    "\n",
    "1. **Load Dataset**: Load the full 1M training samples from Hugging Face\n",
    "2. **Split Dataset**: Create train/validation split using stratified sampling (maintains class balance)\n",
    "3. **Format Prompts**: Convert data into instructional prompts with improved structure\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Load and Split Dataset\n",
    "\n",
    "**Why Stratified Split?**\n",
    "- Ensures training and validation sets have the same True/False ratio\n",
    "- Prevents bias in validation metrics\n",
    "- Better representation of the overall dataset distribution\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Format Training Prompts\n",
    "\n",
    "**Key Improvements Over Baseline:**\n",
    "\n",
    "1. **Includes Student Answer**: The baseline prompt only had Question and Solution, but the task requires comparing the student's answer with the correct solution. This is critical!\n",
    "2. **Emphasizes Reasoning**: The prompt explicitly asks the model to analyze step-by-step reasoning from the solution\n",
    "3. **Clear Task Definition**: Better structure helps the model understand what it needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvAF1sBx8ksx",
    "outputId": "fbecb120-e40d-4918-fd6f-0026d654166c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training samples: 81,000\n",
      "‚úÖ Validation samples: 9,000\n",
      "‚ö° Try 1.7: Using 90,000 samples (9% of full dataset) for fast training\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the full training dataset (1M samples)\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "\n",
    "# Encode is_correct as ClassLabel for stratified split\n",
    "# This ensures the train/val split maintains the same True/False ratio\n",
    "full_dataset = full_dataset.class_encode_column(\"is_correct\")\n",
    "\n",
    "# ‚ö° Try 2: Limit to 90,000 samples for faster training (9% of full dataset)\n",
    "full_dataset = full_dataset.shuffle(seed=42).select(range(90000))\n",
    "\n",
    "# Stratified split: 90% training, 10% validation (maintains class balance)\n",
    "# Using reasonable validation set for model evaluation\n",
    "split_dataset = full_dataset.train_test_split(\n",
    "    test_size=0.1,  # 10% for validation (9,000 samples)\n",
    "    seed=42,\n",
    "    stratify_by_column=\"is_correct\"\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]  # 81,000 samples\n",
    "validation_dataset = split_dataset[\"test\"]  # 9,000 samples\n",
    "\n",
    "print(f\"‚úÖ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"‚úÖ Validation samples: {len(validation_dataset):,}\")\n",
    "print(f\"‚ö° Try 2: Using 90,000 samples (9% of full dataset) for fast training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "88dd481eaf7449efb37ea00515fd819a",
      "470fb1b1ae1043ccbb879db6d870cc90",
      "dee969ca00894933b1e590c4aa541748",
      "9d707f675f174ed7a18b780d4aa46b75",
      "46ba8beb186d498c8a47a05e75ae9678",
      "96a2a87e22344cbbaa3023e1dd9ac570",
      "7cf92420d01f4f05b0152ef15c495593",
      "7f57934740684044a644176454c87d08",
      "be966a385fbd4d7db72e628ba1c2d2a3",
      "d0362d8d165b470e8ae4dc0016f4d7d1",
      "1401409c17114cd38052dd9ecd318483",
      "3e61390214e24399a3ded296f1f0a6b7",
      "591744a9b4eb405bbb87de839db5d762",
      "b80500f9b53d468899faf382948e6203",
      "0fef8561d81b452487e4b5de34dd40ab",
      "210616af422a48b48c67dcaaad5e1a95",
      "368a91bda407440f899efb1a2e02806d",
      "228b6da5ac114dba8199533d30c6c791",
      "35a5177f710c4d4b9caf0c4b3841ce6d",
      "eb0380c89bb74be3963bb4ab98ecbba4",
      "3b59a8684de2483c9c56e3baff95bd55",
      "c0d96778b25b450f8d4c56a35b9457b5"
     ]
    },
    "id": "lEaRjozB3tz8",
    "outputId": "2d73ba9b-03eb-49a3-c740-572c66326e0b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dd481eaf7449efb37ea00515fd819a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e61390214e24399a3ded296f1f0a6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets formatted: 81,000 training, 9,000 validation\n"
     ]
    }
   ],
   "source": [
    "# Improved prompt template emphasizing solution reasoning\n",
    "training_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "{}\"\"\"\n",
    "\n",
    "# EOS token to mark completion\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Format data samples into the prompt template\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format dataset examples into training prompts.\n",
    "\n",
    "    Args:\n",
    "        examples: Batch of dataset examples containing question, solution, answer, is_correct\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with formatted text prompts\n",
    "    \"\"\"\n",
    "    questions = examples[\"question\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    answers = examples[\"answer\"]  # Student answers\n",
    "    outputs = examples[\"is_correct\"]\n",
    "    texts = []\n",
    "    for question, solution, answer, output in zip(questions, solutions, answers, outputs):\n",
    "        # Format the prompt with all components and add EOS token\n",
    "        text = training_prompt.format(\n",
    "            question,\n",
    "            str(solution),\n",
    "            str(answer),\n",
    "            str(output)\n",
    "        ) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Apply formatting to both training and validation datasets\n",
    "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "formatted_validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"‚úÖ Datasets formatted: {len(formatted_train_dataset):,} training, {len(formatted_validation_dataset):,} validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "## **Step 5: Configure LoRA**\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** allows us to efficiently fine-tune the model by training only a small number of adapter parameters instead of the full model.\n",
    "\n",
    "**Try 2 Settings**:\n",
    "- **Rank 20**: Good capacity for 90K dataset\n",
    "- **Alpha 40**: Typically set to 2√órank for optimal scaling\n",
    "- **Dropout 0.1**: Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVZHQ4y74BCG",
    "outputId": "554cc87f-bc73-4303-b7a9-945fba9cc533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,428,800 || all params: 8,082,690,048 || trainable%: 0.6487\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA with Try 2 parameters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 20,  # LoRA rank for good capacity\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 40,  # Typically 2√órank\n",
    "    lora_dropout = 0.1,  # Regularization\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    ")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 6: Set Up SFTTrainer**\n",
    "\n",
    "Configure the training process with **Try 2 Training Settings**:\n",
    "- **1 epoch**: Single pass through 81K training dataset\n",
    "- **Learning rate 1e-4**: Stable learning rate for good convergence\n",
    "- **Warmup 100 steps**: Shorter warmup to start learning faster\n",
    "- **Batch size 8**: Per-device batch size\n",
    "- **Gradient accumulation 8**: Effective batch size of 64 (8 √ó 8)\n",
    "- **No validation during training**: For maximum speed (validation checked separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVrNhZ4y4zsK",
    "outputId": "45357dad-c690-4378-b93e-3ecafd460823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Try 1.7 Trainer configured: 9K samples, LR 1e-4, Warmup 100\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,  # Try 2: Batch size 8\n",
    "        gradient_accumulation_steps = 8,  # Effective batch size: 64\n",
    "        warmup_steps = 100,  # Try 2: Quick warmup\n",
    "        num_train_epochs = 1,  # Single epoch\n",
    "        learning_rate = 1e-4,  # Try 2: Stable learning rate\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 100,\n",
    "        eval_strategy = \"no\",  # Disable validation during training\n",
    "        save_strategy = \"epoch\",  # Save only at end of each epoch\n",
    "        save_total_limit = 2,  # Keep only 2 checkpoints\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Try 2 Trainer configured: 81K training samples, 1 epoch, LR 1e-4, Batch 8√ó8=64, Warmup 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 7: Start Training**\n",
    "\n",
    "Train the model for **1 epoch** over **81,000 training samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "agvQR_Ku5wWY",
    "outputId": "25202315-27af-4dd3-d3b2-e51c3e6f4f58"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 81,000 | Num Epochs = 1 | Total steps = 1,266\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 8 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 52,428,800 of 8,082,690,048 (0.65% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1266' max='1266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1266/1266 4:06:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.583500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.565400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.552400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.474400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1266, training_loss=0.5521391616990028, metrics={'train_runtime': 14803.2622, 'train_samples_per_second': 5.472, 'train_steps_per_second': 0.086, 'total_flos': 2.0443349719221535e+18, 'train_loss': 0.5521391616990028, 'epoch': 1.0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 8: Prepare for Inference**\n",
    "\n",
    "Prepare the trained model for faster inference and test on a validation example to verify it's working correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lvcDSh0JZYm",
    "outputId": "c8b973f0-30a3-4132-dfd7-44c021962bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### QUESTION ####\n",
      "Find the product of $218_9 \\cdot 5_9$.  Express your answer in base 9.\n",
      "\n",
      "#### SOLUTION ####\n",
      "We will first convert the numbers to their decimal equivalents, then find the product using sympy, and finally convert the result back to base $9$.\n",
      "<llm-code>\n",
      "import sympy as sp\n",
      "\n",
      "# convert the numbers...\n",
      "\n",
      "#### STUDENT ANSWER ####\n",
      "12.7448559670782\n",
      "\n",
      "#### MODEL'S PREDICTION ####\n",
      "0<|end_of_text|>\n",
      "\n",
      "#### CORRECT ANSWER ####\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Improved inference prompt (matching training prompt structure)\n",
    "inference_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# Test on a validation example\n",
    "example = validation_dataset[1]\n",
    "question = example[\"question\"]\n",
    "solution = example[\"solution\"]\n",
    "answer = example[\"answer\"]\n",
    "\n",
    "# Format the prompt\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt.format(question, str(solution), str(answer))],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate prediction\n",
    "outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "# Display results\n",
    "print(\"#### QUESTION ####\")\n",
    "print(question)\n",
    "print(\"\\n#### SOLUTION ####\")\n",
    "print(solution[:200] + \"...\" if len(str(solution)) > 200 else solution)\n",
    "print(\"\\n#### STUDENT ANSWER ####\")\n",
    "print(answer)\n",
    "print(\"\\n#### MODEL'S PREDICTION ####\")\n",
    "output_part = response.split(\"Output:\\n\")[-1]\n",
    "print(output_part[:50])\n",
    "print(\"\\n#### CORRECT ANSWER ####\")\n",
    "print(example[\"is_correct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_608V-_NK3B",
    "outputId": "45449901-f0ee-4d74-a91c-99978e9453bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8292222222222222\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Improved inference prompt (matching training prompt structure)\n",
    "inference_prompt = \"\"\"You are an expert mathematician verifying student answers.\n",
    "\n",
    "Your task: Determine if the student's answer is correct by analyzing the problem, the provided solution's reasoning steps, and the student's answer.\n",
    "\n",
    "Question:\n",
    "{}\n",
    "\n",
    "Correct Solution (with step-by-step reasoning):\n",
    "{}\n",
    "\n",
    "Student Answer:\n",
    "{}\n",
    "\n",
    "Based on the solution's reasoning, is the student's answer correct?\n",
    "Respond with:\n",
    "- 'True' if the answer is correct\n",
    "- 'False' if the answer is incorrect\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "count = 0\n",
    "\n",
    "# Test on a all validation examples\n",
    "for i in range(len(validation_dataset)):\n",
    "  example = validation_dataset[i]\n",
    "  question = example[\"question\"]\n",
    "  solution = example[\"solution\"]\n",
    "  answer = example[\"answer\"]\n",
    "\n",
    "  # Format the prompt\n",
    "  inputs = tokenizer(\n",
    "      [inference_prompt.format(question, str(solution), str(answer))],\n",
    "      return_tensors=\"pt\",\n",
    "      truncation=True,\n",
    "      max_length=max_seq_length\n",
    "  ).to(\"cuda\")\n",
    "\n",
    "  # Generate prediction\n",
    "  outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "  response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "  if response.split(\"Output:\\n\")[-1].split(\"<\")[0] == '':\n",
    "    continue\n",
    "\n",
    "  if example[\"is_correct\"] == int(response.split(\"Output:\\n\")[-1].split(\"<\")[0]):\n",
    "    count += 1\n",
    "\n",
    "# Compute the prediction accuracy on the validation examples\n",
    "print(f\"Accuracy: {count/len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Sau2baFPJ7h",
    "outputId": "0f11f8b8-b316-434d-b7a0-f13d04c605ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 23820.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data saved to 'test_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "\n",
    "questions = []\n",
    "solutions = []\n",
    "answers = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    questions.append(question)\n",
    "    solutions.append(solution)\n",
    "    answers.append(answer)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Question': questions,\n",
    "    'Solution': solutions,\n",
    "    'Answer': answers\n",
    "})\n",
    "\n",
    "df.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(f\"Test data saved to 'test_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5489c0ed"
   },
   "source": [
    "## **Step 9: Generate Submission File**\n",
    "\n",
    "Generate predictions for all test samples and create the submission CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e020e6b",
    "outputId": "6c16dad4-c5c2-4977-a82a-a7436e7b7f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 10,000 test samples...\n",
      "Using constrained decoding with allowed tokens: ['True', 'False', '1', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [32:40<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Submission file created successfully!\n",
      "   Total predictions: 10,000\n",
      "   True predictions: 3,570\n",
      "   False predictions: 6,430\n",
      "\n",
      "üìÅ File saved as 'submission.csv'\n",
      "   You can now download this file and submit it to the Kaggle competition.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Constrained Decoding: Force model to generate only \"True\"/\"False\" or \"1\"/\"0\" tokens\n",
    "class AllowedTokensLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"Logits Processor that forces model to generate only allowed tokens\"\"\"\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed = set(allowed_token_ids)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Set probability of disallowed tokens to -inf\n",
    "        mask = torch.full_like(scores, float(\"-inf\"))\n",
    "        for tid in self.allowed:\n",
    "            if tid < scores.shape[-1]:\n",
    "                mask[..., tid] = 0.0\n",
    "        return scores + mask\n",
    "\n",
    "def get_allowed_token_ids(tokenizer):\n",
    "    \"\"\"Return token IDs for allowed tokens (True/False or 1/0)\"\"\"\n",
    "    tokens = [\"True\", \"False\", \"1\", \"0\"]\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id != tokenizer.unk_token_id:\n",
    "            ids.append(token_id)\n",
    "    return ids\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "predictions = []\n",
    "\n",
    "# Setup constrained decoding\n",
    "allowed_ids = get_allowed_token_ids(tokenizer)\n",
    "logits_proc = AllowedTokensLogitsProcessor(allowed_ids)\n",
    "\n",
    "# Optimized Generation configuration\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=1,        # Reduced from 8 to 1 (faster and more accurate)\n",
    "    do_sample=False,          # Deterministic generation\n",
    "    temperature=0.0,          # Use probability distribution as-is\n",
    "    top_p=1.0,\n",
    "    eos_token_id=[tokenizer.eos_token_id],\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Generate predictions for all test samples\n",
    "print(f\"Generating predictions for {len(test_dataset):,} test samples...\")\n",
    "allowed_tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in allowed_ids if tid != tokenizer.unk_token_id]\n",
    "print(f\"Using constrained decoding with allowed tokens: {allowed_tokens}\")\n",
    "\n",
    "for i, example in enumerate(tqdm(test_dataset)):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]  # Student answer\n",
    "\n",
    "    # Format the prompt\n",
    "    prompt = inference_prompt.format(question, str(solution), str(answer))\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate prediction with constrained decoding\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=gen_config,\n",
    "        logits_processor=[logits_proc],  # Apply constrained decoding\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    # Decode only newly generated tokens (more accurate parsing)\n",
    "    new_tokens = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    response_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Check first character only (0 or 1, True/False)\n",
    "    ch = response_text.strip()[:1] if response_text.strip() else \"0\"\n",
    "\n",
    "    # Parse: \"1\" or first letter \"T\" of \"True\" means True\n",
    "    if ch.lower() in [\"1\", \"t\"]:\n",
    "        prediction = True\n",
    "    else:\n",
    "        prediction = False\n",
    "\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Submission file created successfully!\")\n",
    "print(f\"   Total predictions: {len(predictions):,}\")\n",
    "print(f\"   True predictions: {sum(predictions):,}\")\n",
    "print(f\"   False predictions: {len(predictions) - sum(predictions):,}\")\n",
    "print(\"\\nüìÅ File saved as 'submission.csv'\")\n",
    "print(\"   You can now download this file and submit it to the Kaggle competition.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}